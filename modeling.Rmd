# Modeling {#models}

Here is a collection of explorations on modeling approaches as they related to electrocardiography and epidemiology. This is a _sandbox_ for developing meta-modeling functions as well.

- Multiple models
- Visualizing multiple models
- New functions to build models in certain sequences

## Modeling Multiple Outcomes and Predictors

A recurrent issue with causality-focused modeling with ECG data is that there are multiple outcomes (different ECG features). For example, in the `card` package, the `geh` dataset contains several ECG features based on [vectorcardiography](#vcg).

```{r}
library(card)
library(tidyverse)
data(geh)
names(geh)
```

The first issue is the causal model, which can be visualized using a directed acyclic graph. The variables of interest are a subset of the dataset. In this case, we're looking at the relationship of diabetes with cardiotoxicity in a very small subset of participants.

```{r}
library(ggdag)

bd <- dagify(
		GEH ~ DM + Age + BMI + HTN + CAD + IR + Sex,
		DM ~ Age + IR + BMI,
		CAD ~ DM + BMI + HTN + Age + Sex,
		HTN ~ Age,
		IR ~ BMI + Age,
		Age ~ Sex,
		exposure = "DM",
		outcome = "GEH"
)

d1 <- ggdag_parents(bd, "DM", layout = "star") + 
	theme_dag() + 
	theme(legend.position = "none") +
	labs(title = "Factors Affecting Diabetes")

d2 <- ggdag_parents(bd, "GEH", layout = "star") + 
	theme_dag() + 
	theme(legend.position = "none") +
	labs(title = "Factors Affecting GEH")

# Combine and plot
gridExtra::grid.arrange(d1, d2, nrow = 1)
```

As we can see, many things effect ECG findings, and a subgroup of those impact diabetes, suggesting a number of potential effect modifiers and potential confounders/mediators. Using a sequential model building method in the `card` package allows for a simple way to perform this analysis. This will build a linear model for each outcome, and repeat the model with an additional covariate in the sequence of listed in the formula.

```{r}
# Select variables
vars <- 
	c("svg_mag", "az_svg", "el_svg", "qrs_tang", "log_auc_qt", "log_wvg", "lab_hba1c", "lab_fasting_bg", "homa", "dm", "age", "bmi", "bmi_cat", "age_cat", "sex", "htn", "cad", "lab_ser_creatinine", "lab_tchol") 
	
test_data <-
	geh %>%
	select(all_of(vars)) %>%
	#na.omit() %>%
	#filter(homa <= 5 * sd(homa, na.rm = TRUE)) %>% # Remove outliers
	mutate(
		bmi_cat = 
			factor(bmi_cat, levels = c(0:3), 
						 labels = c("Underweight", "Normal", "Overweight", "Obese")),
		age_cat = 
			factor(age_cat, levels = c(0:2), 
						 labels = c("<45", "45-65", ">65")),
		sex = factor(sex, levels = c(0,1), labels = c("Female", "Male"))
	) %>%
	mutate(across(
		c(svg_mag, az_svg, el_svg, qrs_tang, log_auc_qt, log_wvg), 
		function(x) {
			as.vector(scale(x, center = TRUE, scale = TRUE))
		}
	))

# Sequential model building
models <- 
	card::build_models(
		svg_mag + az_svg + el_svg + qrs_tang + log_auc_qt + log_wvg ~ 
			lab_hba1c + age + sex + bmi + cad + htn, 
		data = test_data,
		exposure = "lab_hba1c",
		engine = "linear",
		type = "sequential"
	)

head(models)
```

To assess or get a sense of how the variables are playing out, we can visualize the estimates across the builds of the models. This will also use the `gganimate` package to show the effect of data layering

```{r, message=FALSE}
# Libraries
library(gganimate)
library(ggthemes)

# Data
df <- 
	models %>%
	# Remove intercepts
	filter(term != "(Intercept)") %>%
	# Sequence the terms
	mutate(
		term = 
			factor(
				term, 
				levels = c("lab_hba1c", "age", "sexMale", "bmi", "cad1", "htn1"),
				labels = c("HbA1c", "Age", "Sex", "BMI", "CAD", "HTN")
			)
	) 

# ggplot
g <- ggplot(df, aes(x = factor(covar), y = estimate, color = term)) + 
	facet_wrap(~outcomes, scales = "fixed") + 
	geom_point(
		aes(color = term), 
		data = filter(df, p.value >= 0.20), 
		shape = 1, 
		position = "jitter"
	) + 
	geom_point(
		aes(color = term), 
		data = filter(df, p.value < 0.20), 
		shape = 19, 
		position = "jitter"
	) + 
	scale_color_ptol(name = "Predictors") + 
	theme_minimal() + 
	theme(
		legend.position = "bottom", legend.box = "horizontal",
		panel.border = element_rect(colour = "black", fill = NA)
	) + 
	labs(
		title = "Estimates in Sequential Models",
		x = "Number of Covariates in Model",
		y = "GEH Parameters (z-normalized)"
	)

# Animated
a <- g + transition_reveal(covar)

animate(a, end_pause = 30)
```

## Building a Modeling Matrix Function

One issue that has occurred is that using the function `build_models()` is the idea that a prespecified formula can be used to generate a large number of models, so we can assess the impact of each variable on the model. However, in R, the limitation is that each regression package is unique in how it is specified. One option is to rely on the prespecified `parsnip` models that unify regression modeling formulas.

### A `tidy` Approach to Multiple Outcomes and Predictors

Here is a base example that I hope to build off of (generated by Julia Silge for an issue filed on the `workflows` package).

```{r}
library(tidymodels)
library(vctrs)
#> Attaching package: 'vctrs'
#> The following object is masked from 'package:tibble':
#> 
#>     data_frame
#> The following object is masked from 'package:dplyr':
#> 
#>     data_frame

outcome <- "mpg"
predictors <- setdiff(names(mtcars), outcome)

# Specify parsnip model to be used
lm_spec <- linear_reg() %>% set_engine("lm")

## make a little function to create a workflow with `mpg` as outcome and our set of predictors
wf_seq <- function(preds) {
  workflow() %>%
    add_model(lm_spec) %>%
    add_variables(outcomes = mpg, predictors = !!preds)
}

## set up the "sequential" set of predictors and create each workflow, then fit
tibble(num_preds = 1:length(predictors)) %>%
  mutate(preds     = map(num_preds, ~vec_slice(predictors, 1:.))) %>%
  mutate(wf        = map(preds, wf_seq),
         fitted_wf = map(wf, fit, mtcars))
#> # A tibble: 10 x 4
#>    num_preds preds      wf         fitted_wf    
#>        <int> <list>     <list>     <list>    
#>  1         1 <chr [1]>  <workflow> <workflow>
#>  2         2 <chr [2]>  <workflow> <workflow>
#>  3         3 <chr [3]>  <workflow> <workflow>
#>  4         4 <chr [4]>  <workflow> <workflow>
#>  5         5 <chr [5]>  <workflow> <workflow>
#>  6         6 <chr [6]>  <workflow> <workflow>
#>  7         7 <chr [7]>  <workflow> <workflow>
#>  8         8 <chr [8]>  <workflow> <workflow>
#>  9         9 <chr [9]>  <workflow> <workflow>
#> 10        10 <chr [10]> <workflow> <workflow>
```

This goes back to the approach listed in [R4DS](https://r4ds.had.co.nz/many-models.html) that shows the `purrr` method of regressions with the `map()` function.

A data frame that described this would be likely the most succinct way to handle this issue, using several specified elements:

- a single dataframe that was made, to be used throughout
- column to describe the number of predictors or to help identify each row
- column that contains the predictors
- column that contains the outcomes (allowing for combinations)
- column that specifies the exposures (or fixed effects)
- column that contains model specifications using `parsnip`
- column that contains workflows

To set this up, likely there will need to be several functions/steps:

1. Creating a modeling "matrix" as above
2. Updating or allowing modifications to the table to occur, such as combining additional matrices, etc
3. Fitting the models using the specified workflows

The primary structure here will be a `tibble` that contains the basic parameters that can be extracted at the end. 

#### Creating a modeling matrix

Here we can see a slighly modified set of sequences to create a data frame that would hold the workflow needed for a regression analysis, but without the actual results or fit.

```{r}

# Would like to use a formula approach to create this matrix
f <- 
	svg_mag + az_svg + el_svg + qrs_tang + log_auc_qt + log_wvg ~
	lab_hba1c + age + sex + bmi + cad + htn 

# Left and right side, and length of each
outcomes <- all.vars(f[[2]])
predictors <- all.vars(f[[3]])
n_outcomes <- length(outcomes)
n_predictors <- length(predictors)

# Assuming this will be a linear regression
lm_mod <- linear_reg() %>% set_engine("lm")

wf_seq <- function(outs, preds, mods) {
  workflow() %>%
    add_model(mods) %>%
    add_variables(outcomes = !!outs, predictors = !!preds)
}

all_models <-
	tibble(n_covar = 1:length(predictors)) %>%
	mutate(pred = map(n_covar, ~vec_slice(predictors, 1:.))) %>%
	expand_grid(., out = outcomes) %>%
	mutate(spec = list(lm_mod)) %>%
	mutate(wf = pmap(list(out, pred, spec), wf_seq))
```

Thus, the modeling matrix is a *plan* that will be used and should likely exist as a moldable object. That object can then be shaped into a table of columns going over the types of models to be run. Finally, the table can be analyzed. This fits the epidemiological or research workflow in that general aims for the research are decided, hypotheses are generated, and finally statistical analysis is performed.

1. `plan()` should take in a formula, specific characteristics, and create a template
1. `update()` should allow the plan to be modified or updated
1. `model()` will allow for fitting the models defined

This is just a first draft at a thoughtful approach, and further iterations should allow for a more fluid, conversational function set.

```{r}
# Data set to be used was generated above
df <- test_data

# The above made modeling table can then be fit
final <-
	all_models %>%
	mutate(fit = map(wf, fit, data = df))
```

