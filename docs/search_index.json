[
["index.html", "Thoughts Preface", " Thoughts Anish Shah 2020-08-31 Preface This initially was designed to help document exploratory concepts that were developed using the card package, but also expanded to clinical concepts leveraging the power of R and Rmarkdown. As this develops, it will likely create its own order. Currently it revolves around the principal components: circadian rhythm electrocardiography autonomic physiology clinical medicine programming "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction An introduction would go here. "],
["r-and-rstudio.html", "1.1 R and RStudio", " 1.1 R and RStudio I have been learning R since 2010, and have found it to be an excellent: open-source excellent community an outstanding IDE with RStudio package development Currently using R version 4.0.0 (2020-04-24), nicknamed Arbor Day, with RStudio 1.3. "],
["git-and-github.html", "1.2 git and Github", " 1.2 git and Github "],
["setup.html", "Chapter 2 Technical Setup", " Chapter 2 Technical Setup A workflow outside of research and medicine should be a natural corollary to it, taking similar approaches to problems to help making things simpler (instead of more complicated). Certain programs, approaches, and philosophies have made this possible, detailed below. vim - text editor mutt - email client git "],
["vim.html", "2.1 Vim", " 2.1 Vim Vim is much more than a text editor. "],
["mutt.html", "2.2 Mutt", " 2.2 Mutt Email clients are typically bloated with additional “features” that are supposed to improve the email experience. However, it seems that these additions in a GUI interface actually slow things down. Mutt, and its heavily-featured fork Neomutt, are ways around those problems with a command line interface. Email is not just a single service, but a bundling of several components. mutt is a MUA (mail user agent) that is a front-end for users to manage stored messages in a mailbox msmtp is a MTA (mail transport agent) that sends mails through SMTP (simple mail transfer protocol) mbsync is a MRA (mail retrieval agent) that can hop onto a mail server and actually retrieve items from inboxes Originally, for my personal gmail and professional/institution, the following resources were helpful. https://wincent.com/blog/email http://stevelosh.com/blog/2012/10/the-homely-mutt/ https://gitlab.com/muttmua/mutt/wikis/MuttGuide https://www.youtube.com/watch?v=2jMInHnpNfQ&amp;t=111s https://webgefrickel.de/blog/a-modern-mutt-setup https://webgefrickel.de/blog/a-modern-mutt-setup-part-two "],
["building-a-cv.html", "2.3 Building a CV", " 2.3 Building a CV Instead of storing the elements of a CV in a WYSIWYG format, an alternative approach is to use separate files that can be compiled together to produce a CV in a data-driven pipeline. This process was inspired by the datadrivencv and vitae packages. Reference links: https://slides.mitchelloharawild.com/vitae/#1 https://github.com/ropensci/rorcid https://github.com/robjhyndman/CV "],
["circadian.html", "Chapter 3 Circadian Physiology ", " Chapter 3 Circadian Physiology "],
["chronobiology.html", "3.1 Chronobiology", " 3.1 Chronobiology Will discuss circadian biology/physiology. "],
["circadian-disruption.html", "3.2 Circadian Disruption", " 3.2 Circadian Disruption "],
["references.html", "3.3 References", " 3.3 References "],
["cosinor.html", "Chapter 4 Cosinor Analysis", " Chapter 4 Cosinor Analysis The issue with time series analysis is that the data is by its nature circular and thus cannot be easily be analyzed through traditional, linear methods. The following is the development/expansion of the cosinor model to help study circadian rhythms (3) using R. The card package was developed to help tackle this problem. # Library library(card) library(tidyverse) ## Warning: package &#39;tidyverse&#39; was built under R version 4.0.2 ## ── Attaching packages ──────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.1 ## ✓ tidyr 1.1.1 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## Warning: package &#39;tibble&#39; was built under R version 4.0.2 ## Warning: package &#39;tidyr&#39; was built under R version 4.0.2 ## Warning: package &#39;dplyr&#39; was built under R version 4.0.2 ## ── Conflicts ─────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() # Dataset data(&quot;twins&quot;) # Example of data ggplot(twins, aes(x = hour, y = rDYX)) + geom_smooth(method = &quot;gam&quot;, se = TRUE) ## `geom_smooth()` using formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; "],
["overview.html", "4.1 Overview", " 4.1 Overview Using the cosinor() function, the characteristics of the circadian pattern can be retrieved. # Cosinor model m &lt;- cosinor(rDYX ~ hour, twins, tau = 24) summary(m) ## Individual Cosinor Model ## ------------------------------------------ ## Call: ## cosinor(formula = rDYX ~ M + A1 * cos(2*pi*hour/24 + phi1) ## ## Period(s): 24 ## ## Residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -3.12633 -0.53228 -0.03597 0.00000 0.49132 4.82150 ## ## Coefficients: ## Estimate Std. Error ## mesor 2.8604855 0.006098624 ## amp1 0.2986101 0.008746706 ## phi1 -2.6687044 0.028860014 The statistical principles behind this method allow for different methods to model, diagnose, and interpret findings. (Refinetti, Cornélissen, and Halberg 2007; Cornelissen 2014) single component cosinor multiple component cosinor population cosinor confidence intervals (ellipse method) zero amplitude test lack-of-fit testing The example here use the dataset twins which contains a continuous ECG signal, called DYX, collected at hourly time points. References "],
["single-component-cosinor.html", "4.2 Single component cosinor", " 4.2 Single component cosinor The single component cosinor method is modeled as: \\[Y(t) = M + A cos(\\frac{2 \\pi t}{\\tau} + \\phi) + \\epsilon\\] Where: \\[ \\begin{aligned} M &amp;= MESOR\\ (midline\\ estimating\\ statistic\\ of\\ rhythm) \\\\ t &amp;= time in hours \\\\ \\epsilon &amp;= error \\\\ \\phi &amp;= acrophase \\\\ \\tau &amp;= tau\\ (period) \\\\ \\end{aligned} \\] To model this function, it must be transformed linearly to assess the coefficients. \\[Y(t) = M + \\beta x_{t} + \\gamma z_{t} + \\epsilon_{t}\\] The new coefficients and parameters are defined as: \\[ \\begin{aligned} \\beta &amp;= A cos(\\phi) \\\\ \\gamma &amp;= -A sin(\\phi) \\\\ x_{t} &amp;= cos(\\frac{2 \\pi t}{\\tau}) \\\\ z_{t} &amp;= sin(\\frac{2 \\pi t}{\\tau}) \\\\ \\end{aligned} \\] In the twins data, the time value \\(t\\) is measured in hours. As this is 24-hour data, the assumption is that \\(\\tau = 24\\). df &lt;- subset(twins, patid == 60) # Single individual cosinor y &lt;- df$rDYX t &lt;- df$hour n &lt;- length(t) # Number of observations period &lt;- 24 # Transformed variables x &lt;- cos(( 2 * pi * t) / period) z &lt;- sin(( 2 * pi * t) / period) To generate the coefficients in R requires sovling a matrix of normal/linear equations. # Matrices ymat &lt;- as.matrix(cbind(y = c(sum(y), sum(y * x), sum(y * z)))) mcol &lt;- c(n, sum(x), sum(z)) # Mesor column bcol &lt;- c(sum(x), sum(x^2), sum(x * z)) # Beta column gcol &lt;- c(sum(z), sum(x * z), sum(z^2)) # Gamma column xmat &lt;- as.matrix(cbind(m = mcol, b = bcol, g = gcol)) # Solution coefs &lt;- solve(t(xmat) %*% xmat, tol = 1e-21) %*% (t(xmat) %*% ymat) mesor &lt;- coefs[1] # mesor beta &lt;- coefs[2] # beta gamma &lt;- coefs[3] # gamma For a single cosinor, as in, the analysis of the values from a single individual over 1 period, the values for the Amplitude (\\(A\\)) and Acrophase (\\(\\phi\\)) can be calculated. \\[ \\begin{aligned} A &amp;= \\sqrt{(\\beta^2 + \\gamma^2)} \\phi &amp;= k \\pi + g \\times arctan(\\frac{\\gamma}{\\beta}) \\end{aligned} \\] Because the values of \\(\\gamma\\) and \\(\\beta\\) represent trigonemtric values, the position or quadrant of the circle changes the value of \\(\\phi\\). \\(\\beta\\) \\(\\gamma\\) k g + + 0 -1 + - \\(-2 \\pi\\) +1 - + \\(- \\pi\\) +1 - - \\(- \\pi\\)$ -1 These calculations were made with the cosinor model seen above. # Amplitude amp &lt;- sqrt(beta^2 + gamma^2) # Acrophase (phi) must be in correct quadrant sb &lt;- sign(beta) sg &lt;- sign(gamma) theta &lt;- atan(abs(gamma / beta)) if ((sb == 1 | sb == 0) &amp; sg == 1) { phi &lt;- -theta } else if (sb == -1 &amp; (sg == 1 | sg == 0)) { phi &lt;- theta - pi } else if ((sb == -1 | sb == 0) &amp; sg == -1) { phi &lt;- -theta - pi } else if (sb == 1 &amp; (sg == -1 | sg == 0)) { phi &lt;- theta - (2 * pi) } cat(paste0(&quot;Amplitude = &quot;, round(amp, 3))) ## Amplitude = 0.168 cat(paste0(&quot;Acrophase = &quot;, round(phi, 3))) ## Acrophase = -2.676 "],
["population-mean-cosinor.html", "4.3 Population-mean cosinor", " 4.3 Population-mean cosinor Based on the work by Cornelissen et al 2014 (Cornelissen 2014), the population mean cosinor can be estimated by applying the single or multiple component cosinor to each individual. \\[\\{\\hat{u} = \\hat{M}_{i} + \\hat\\beta_{i} + \\hat\\gamma_{i} + ... \\}\\] Where \\(i = 1, 2, ..., k\\) for each individual contribution to the population cosinor metrics. Each parameter can then be “averaged” to estimate the population parameters. This allows extension from a single individual to populations, particularly research studies with cohorts of patients. The \\(A\\) and \\(\\phi\\) however are calculated using the previous equations but through the \\(\\mu_{\\beta}\\) and \\(\\mu_{\\gamma}\\) values. The MESOR can be calculated simply by measure the mean value from each sample (\\(MESOR_{population} = MESOR_{1} + ... + MESOR_{k}\\)). # Parameters for population mean cosinor, using best datasets df &lt;- twins %&gt;% filter(med_beta_blockers != 1) %&gt;% select(c(&quot;rDYX&quot;, &quot;hour&quot;, &quot;patid&quot;)) names(df) &lt;- c(&quot;y&quot;, &quot;t&quot;, &quot;pop&quot;) highCounts &lt;- df %&gt;% group_by(pop) %&gt;% tally() %&gt;% filter(n &gt; 20) # Subset for full data df &lt;- subset(df, pop %in% highCounts$pop) # Number of individuals k &lt;- length(unique(df$pop)) # Individual cosinor models are implemented for each individual kCosinors &lt;- with( df, by(df, pop, function(x) { cosinor(y ~ t, data = x, tau = 24) }) ) # The coefficients have to be extracted and summarized tbl &lt;- sapply(kCosinors, stats::coef) coef_names &lt;- c(&quot;mesor&quot;, &quot;amp&quot;, &quot;phi&quot;, &quot;beta&quot;, &quot;gamma&quot;) rownames(tbl) &lt;- coef_names xmat &lt;- t(tbl) # Get mean for each parameter (mesor, beta, gamma), ignoring averaged amp/phi coefs &lt;- apply(xmat, MARGIN = 2, function(x) { sum(x) / k }) mesor &lt;- unname(coefs[&quot;mesor&quot;]) beta &lt;- unname(coefs[&quot;beta&quot;]) gamma &lt;- unname(coefs[&quot;gamma&quot;]) # Get amplitude amp &lt;- sqrt(beta^2 + gamma^2) # Acrophase (phi) must be in correct quadrant sb &lt;- sign(beta) sg &lt;- sign(gamma) theta &lt;- atan(abs(gamma / beta)) if ((sb == 1 | sb == 0) &amp; sg == 1) { phi &lt;- -theta } else if (sb == -1 &amp; (sg == 1 | sg == 0)) { phi &lt;- theta - pi } else if ((sb == -1 | sb == 0) &amp; sg == -1) { phi &lt;- -theta - pi } else if (sb == 1 &amp; (sg == -1 | sg == 0)) { phi &lt;- theta - (2 * pi) } # Update coefficients coefs[&quot;amp&quot;] &lt;- amp coefs[&quot;phi&quot;] &lt;- phi # Updated coefficients names(coefs) &lt;- coef_names print(coefs) ## mesor amp phi beta gamma ## 2.9020896 0.3144840 -2.6948505 -0.2836203 0.1358664 4.3.1 Confidence Intervals for Population Cosinor The confidence intervals for a population are more complicated to generate, and several approaches are documented in the literature. 4.3.1.1 Ellipsoid Approach The values, including standard deviation and standard error for the MESOR are calculated using standard statistics along a t-distribution, with degree of freedom based on number of observations. In this case, \\(\\alpha = 0.05\\). # Standard error for mesor kcoefs &lt;- data.frame(xmat) se &lt;- sd(kcoefs$mesor) / sqrt(k - 1) cat(round(se, 3)) ## 0.022 The statistical parameters around the \\(A\\) and \\(\\phi\\) are more complex, as they are joined together, and represent a joint confidence region of the substitute parameters \\(\\beta\\) and \\(\\gamma\\). The first step is the calculation of the variance and covariance of \\(\\beta\\) and \\(\\gamma\\). This can be used to generated teh standard deviation of these variables. \\[ \\begin{aligned} \\sigma_{\\beta \\gamma} &amp;= \\sqrt{COV_{\\beta \\gamma}} \\\\ \\sigma_{\\beta} &amp;= \\sqrt{VAR_{\\beta}} \\\\ \\sigma_{\\gamma} &amp;= \\sqrt{VAR_{\\gamma}} \\\\ \\end{aligned} \\] sbg &lt;- sqrt(cov(kcoefs$beta, kcoefs$gamma)) sb &lt;- sqrt(var(kcoefs$beta)) sg &lt;- sqrt(var(kcoefs$gamma)) The next step is the creation of a confidence ellipse for a given confidence interval. This ellipse is defined by all points \\((\\beta*, \\gamma*)\\) that satisfy the elliptical equation. \\[ \\frac{(\\beta - \\beta*)^2}{\\sigma^2_{\\beta}} - \\frac{2r(\\beta - \\beta*)(\\gamma - \\gamma*)}{\\sigma_{\\beta} \\sigma_{\\gamma}} + \\frac{(\\gamma - \\gamma*)}{\\sigma^2_{\\gamma}} = \\frac{2(1 - r^2)(k - 1)F_{1 - \\alpha}}{k(k - 2)} \\] This can be reorganized/reorderd by solving for a single parameter first, such as \\(\\beta*\\), which will lead to two potential values. \\[ \\beta* = \\frac{ \\beta \\sigma_{\\gamma} - r \\sigma_{beta} \\gamma + r \\sigma_{beta} \\gamma* \\pm \\sqrt{(r^2 - 1)[(\\gamma* - \\gamma)^2 - \\frac{2(k-1)}{k(k-2)}F_{1-\\alpha}\\sigma^2_{\\gamma}]} } {\\sigma_{\\gamma}} \\] … where \\(r = \\frac{\\sigma_{\\beta \\gamma}}{\\sigma_{\\beta} \\sigma_{\\gamma}}\\) This is calculated using the above equation for a potential sequence of values of \\(\\beta*\\) and \\(\\gamma*\\). THe constant values are already known to us, including the \\(\\beta\\) and \\(\\gamma\\) variables. # Variance/covariance and initial values were found above # Define new constants alpha &lt;- 0.05 r &lt;- sbg / (sb * sg) fstat &lt;- qf(1 - alpha, 2, k - 2) # Sequence values gseq &lt;- seq(from = -abs(gamma*3), to = abs(gamma*3), length.out = 100) bpos &lt;- ((beta * sg) - (r * sb * gamma) + (r * sb * gseq) + (sb * sqrt(as.complex((r^2 - 1) * ((gseq - gamma)^2 - ((2 * (k - 1)) / (k * (k - 2)) * fstat * sg^2)))))) / sg bneg &lt;- ((beta * sg) - (r * sb * gamma) + (r * sb * gseq) - (sb * sqrt(as.complex((r^2 - 1) * ((gseq - gamma)^2 - ((2 * (k - 1)) / (k * (k - 2)) * fstat * sg^2)))))) / sg # Restrict to only real numbers (not complex/imaginary) index &lt;- Im(bpos) == 0 | Im(bpos) == Im(bneg) # values are zero in both are REAL numbers gseq &lt;- Re(gseq[index]) bpos &lt;- Re(bpos[index]) bneg &lt;- Re(bneg[index]) # Plot out ellipse ggplot() + # Original values geom_point(aes(x = gamma, y = beta), data = kcoefs, alpha = 0.2) + # Potential ellipse versus hyperbola geom_point(aes(x = gseq, y = bpos), col = &quot;red&quot;, size = 0.5) + geom_point(aes(x = gseq, y = bneg), col = &quot;blue&quot;, size = 0.5) + # Predicted segment geom_segment(aes(x = 0, y = 0, xend = -amp*sin(phi), yend = amp*cos(phi)), size = 1.5) + # Axes geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + xlim(-abs(gamma)*5, abs(gamma)*5) + ylim(-abs(beta)*5, abs(beta)*5) ## Warning: Removed 7 rows containing missing values (geom_point). ## Warning: Removed 33 rows containing missing values (geom_point). # Using {car} border &lt;- car::dataEllipse(cbind(kcoefs$gamma, kcoefs$beta), levels = 0.95) %&gt;% as_tibble() ggplot() + geom_point(aes(x = x, y = y), data = border, col = &quot;red&quot;) + geom_point(aes(x = gamma, y = beta), data = kcoefs, alpha = 0.5) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) 4.3.1.2 Sampling Matrix Approach An approach, according to Bingham et al 1982, is to use the sampling matrix, generated from the following formulas and calculated below. The key formulas for the population cosinor confidence intervals are: $$ \\begin{aligned} MESOR &amp; \\ A &amp;t_{1 - /2} \\ &amp;+ arctan() \\end{aligned} $$ Where the matrix variables are shown below: $$ \\begin{aligned} s_{22} &amp;= \\ s_{23} &amp;= \\ s_{33} &amp;= \\end{aligned} $$ Thus, we can use these equations to calculate the confidence intervals. # Stats alpha &lt;- 0.05 tdist &lt;- qt(1 - alpha/2, k - 1) # Matrix variables s22 &lt;- ((sb^2 * beta^2) + (2 * sbg * beta * gamma) + (sg^2 * gamma^2)) / (k * amp^2) s23 &lt;- (-1 * (sb^2 - sg^2) * (beta * gamma) + sbg * (beta^2 - gamma^2)) / (k * amp^2) s33 &lt;- ((sb^2 * gamma^2) - (2 * sbg * beta * gamma) + (sg^2 * beta^2)) / (k * amp^2) 4.3.1.3 Approach by Fernandez (Fernández, Mojón, and Hermida 2004) The population aproach can also be predicted through an alternative, perhaps more intuitive way. If normality is assumed, the estimated parameters can be generated from the individual parameters, similar to the MESOR, in a single population, and allows for simple statistical testing between populations. (Fernández, Mojón, and Hermida 2004) # Stats kcoefs &lt;- data.frame(xmat) alpha &lt;- 0.05 tdist &lt;- qt(1 - alpha/2, k - 1) # Plot g &lt;- ggplot() + geom_segment( aes( x = gamma - (tdist * sd(kcoefs$gamma) / sqrt(k)), xend = gamma + (tdist * sd(kcoefs$gamma) / sqrt(k)), y = 0, yend = 0 ), col = &quot;cornflowerblue&quot;, size = 2 ) + geom_segment( aes( y = beta - (tdist * sd(kcoefs$beta) / sqrt(k)), yend = beta + (tdist * sd(kcoefs$beta) / sqrt(k)), x = 0, xend = 0 ), col = &quot;indianred&quot;, size = 2 ) + geom_rect( aes( ymin = beta - (tdist * sd(kcoefs$beta) / sqrt(k)), ymax = beta + (tdist * sd(kcoefs$beta) / sqrt(k)), xmin = 0, xmax = gamma + (tdist * sd(kcoefs$gamma) / sqrt(k)) ), fill = &quot;indianred&quot;, alpha = 0.5 ) + geom_rect( aes( ymin = 0, ymax = beta - (tdist * sd(kcoefs$beta) / sqrt(k)), xmin = gamma - (tdist * sd(kcoefs$gamma) / sqrt(k)), xmax = gamma + (tdist * sd(kcoefs$gamma) / sqrt(k)) ), fill = &quot;cornflowerblue&quot;, alpha = 0.5 ) + geom_point(aes(x = gamma, y = beta), size = 2) + geom_segment( aes( x = 0, y = 0, xend = -amp*sin(phi), yend = amp*cos(phi) ), size = 1.2 ) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) # Values possible for amplitude is.logical(round(kcoefs$amp * cos(kcoefs$phi), 3) == round(kcoefs$beta, 3)) ## [1] TRUE # Values for phi is.logical(round(kcoefs$amp * -1 * sin(kcoefs$phi), 3) == round(kcoefs$gamma, 3)) ## [1] TRUE sd(kcoefs$beta) ## [1] 0.3023976 sd(kcoefs$gamma) ## [1] 0.2376721 However, this method is more complicated when multiple components are included. References "],
["multiple-component-cosinor.html", "4.4 Multiple-Component Cosinor", " 4.4 Multiple-Component Cosinor Fitting physiological/circadian data may involve other patterns than a single, 24-hour frequency. There may be additional components or cosine waves that better explain the datasets, such as at 8 hours (e.g. meal times). Thus, it can be beneficial to add a secondary component. Implementing this in R is made complex as it now uses a variable number of inputs and variable number of outputs. This is performed through the paradigm suggested in the hardhat package, with a user-facing and computational side. The input data is given in the function call, then bridged to the model implementation. \\[ \\begin{aligned} y(t) &amp;= M + \\sum_{j}(A_{j} cos(2 \\pi t / \\tau_{j} + \\phi_{j}) \\\\ y(t) &amp;= M + \\beta_{j} x_{j} + \\gamma_{j} z_{j} \\end{aligned} \\] Where \\(j = 1,\\ 2,\\ ... p\\), which is the number of parameters. In a single component cosinor, there are at 3 parameters: \\(2(p=1) + 1\\), and in a multiple component: \\(2(p=j) + 1\\). The components are based on the periods. In a formula based approach… # This is the final dataset for a single component, however we have multiple object &lt;- cosinor(rDYX ~ hour, data = twins, tau = c(24, 8)) cat(object$call) ## cosinor(formula = rDYX ~ M + A1 * cos(2*pi*hour/24 + phi1) + A2 * cos(2*pi*hour/8 + phi2) # Periods tau &lt;- c(24, 8) # Two components l &lt;- length(tau) j &lt;- 2*l + 1 period &lt;- tau # No variable parameters y &lt;- outcomes &lt;- twins$rDYX t &lt;- predictors &lt;- twins$hour n &lt;- length(t) # Need to create number of x values to match number of periods # x1, x2, z1, z2 in this case for(i in 1:l) { assign(paste0(&quot;x&quot;, i), cos((2 * pi * t) / period[i])) assign(paste0(&quot;z&quot;, i), sin((2 * pi * t) / period[i])) } # Creating a new dataframe with all variables model &lt;- data.frame(y, t, mget(paste0(&quot;x&quot;, 1:l)), mget(paste0(&quot;z&quot;, 1:l))) # The formula, where the intercept will be the MESOR (not included) f &lt;- as.formula( paste0(&quot;y ~ &quot;, paste0(&quot;x&quot;, 1:l, &quot; + &quot;, &quot;z&quot;, 1:l, collapse = &quot; + &quot;)) ) print(f) ## y ~ x1 + z1 + x2 + z2 # Can create a model frame here using two approaches # Base R and with hardhat m &lt;- model.frame(f, model) xmat &lt;- model.matrix(f, m) ymat &lt;- as.matrix(y) # Hardhat framed &lt;- hardhat::model_frame(f, model) mat &lt;- hardhat::model_matrix(framed$terms, framed$data) # Solve for coefficients, including amplitude and acrophase coefs &lt;- solve(t(xmat) %*% xmat) %*% t(xmat) %*% ymat mesor &lt;- coefs[1] for(i in 1:l) { # Beta and gamma terms assign(paste0(&quot;beta&quot;, i), unname(coefs[paste0(&quot;x&quot;, i),])) assign(paste0(&quot;gamma&quot;, i), unname(coefs[paste0(&quot;z&quot;, i),])) # Amplitude assign(paste0(&quot;amp&quot;, i), sqrt(get(paste0(&quot;beta&quot;, i))^2 + get(paste0(&quot;gamma&quot;, i))^2)) # Phi / acrophase sb &lt;- sign(get(paste0(&quot;beta&quot;, i))) sg &lt;- sign(get(paste0(&quot;gamma&quot;, i))) theta &lt;- atan(abs(get(paste0(&quot;gamma&quot;, i)) / get(paste0(&quot;beta&quot;, i)))) if ((sb == 1 | sb == 0) &amp; sg == 1) { phi &lt;- -theta } else if (sb == -1 &amp; (sg == 1 | sg == 0)) { phi &lt;- theta - pi } else if ((sb == -1 | sb == 0) &amp; sg == -1) { phi &lt;- -theta - pi } else if (sb == 1 &amp; (sg == -1 | sg == 0)) { phi &lt;- theta - (2 * pi) } assign(paste0(&quot;phi&quot;, i), phi) } coefs &lt;- unlist(c(mesor = mesor, mget(paste0(&quot;amp&quot;, 1:l)), mget(paste0(&quot;phi&quot;, 1:l)), mget(paste0(&quot;beta&quot;, 1:l)), mget(paste0(&quot;gamma&quot;, 1:l)))) In a multiple-component cosinor analysis, if the periods are harmonic, as in if the longest value of \\(\\tau\\) is an integer multiple of the shortest \\(\\tau\\) (fundamental frequency), additional features can be extracted from the fit. \\(A_{g}\\) - global amplitude, defined as half of the difference between peak and trough values \\(\\phi_{O}\\) - orthophase, defined as lag time to peak value \\(\\phi_{B}\\) - bathyphase, defined as lag time to trough value This can be assessed through the fitted values in an augmented cosinor object. # Multiple component object object &lt;- cosinor(rDYX ~ hour, data = twins, tau = c(24, 12)) # Retrieve parameter values and fit aug &lt;- augment(object) fit &lt;- unique(aug[c(&quot;t&quot;, &quot;.fitted&quot;)]) mesor &lt;- object$coefficients[1] # Orthophase peak &lt;- max(fit$.fitted) orthophase &lt;- fit$t[which.max(fit$.fitted)] # Bathyphase trough &lt;- min(fit$.fitted) bathyphase &lt;- fit$t[which.min(fit$.fitted)] # Global amplitude globalAmp &lt;- (peak - trough) / 2 # Reference phase zero &lt;- min(aug$t) # Plot ggplot(fit, aes(x = t, y = .fitted)) + stat_smooth(method = &quot;gam&quot;, color = &quot;black&quot;, se = FALSE, size = 1.2) + # Mesor geom_hline(yintercept = mesor, color = &quot;grey&quot;) + geom_text(x = zero + 1, y = mesor + 0.01*mesor, label = &quot;MESOR&quot;) + # Orthophase geom_vline(xintercept = orthophase, color = &quot;grey&quot;) + geom_point(aes(x = orthophase, y = peak), size = 2) + geom_segment( aes(x = zero, xend = orthophase, y = peak, yend = peak), linetype = &quot;dotted&quot;, size = 0.8 ) + geom_text( aes(x = (orthophase - zero) / 2, y = peak + 0.01*mesor), label = &quot;Orthophase&quot; ) + # Bathyphase geom_vline(xintercept = bathyphase, color = &quot;grey&quot;) + geom_point(aes(x = bathyphase, y = trough), size = 2) + geom_segment( aes(x = zero, xend = bathyphase, y = trough, yend = trough), linetype = &quot;dotted&quot;, size = 0.5 ) + geom_text( aes(x = (bathyphase + zero) / 2, y = trough - 0.01*mesor), label = &quot;Bathyphase&quot; ) + # Global Amplitude geom_segment( aes(x = orthophase, xend = (orthophase + bathyphase)/2, y = peak, yend = peak), linetype = &quot;twodash&quot;, size = 0.5 ) + geom_segment( aes(x = bathyphase, xend = (orthophase + bathyphase)/2, y = trough, yend = trough), linetype = &quot;twodash&quot;, size = 0.5 ) + geom_segment( aes( x = (orthophase + bathyphase)/2, xend = (orthophase + bathyphase)/2, y = mesor, yend = trough ), linetype = &quot;twodash&quot;, size = 0.5, arrow = arrow(type = &quot;closed&quot;, length = unit(0.03, &quot;npc&quot;)) ) + geom_segment( aes( x = (orthophase + bathyphase)/2, xend = (orthophase + bathyphase)/2, y = mesor, yend = peak ), linetype = &quot;twodash&quot;, size = 0.5, arrow = arrow(type = &quot;closed&quot;, length = unit(0.03, &quot;npc&quot;)) ) + geom_text( aes(x = (bathyphase + orthophase)/2 + 4, y = 1.01*mesor), label = &quot;2 x Global Amplitude&quot; ) + theme_minimal() ## `geom_smooth()` using formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; "],
["heart.html", "Chapter 5 Heart", " Chapter 5 Heart As a physician, the mental model of disease is a human substrate that has been altered or damaged, such as a weakened heart after a myocardial infarction. Its an intuitive concept that is built on experience that stems from the underlying clinical research. That idea of mapping a disease onto a substrate allows for a certain simplicity in explaining a disease - physician’s teach via metaphor. The heart is metaphorically a house. It has rooms that are separated by doors, it has pipes in the walls, and electricity running through it. plumbing: epicardial and resistance vessels electric: sinoatrial and atrioventricular nodes and their corresponding fibers structure: the size and shape of the chambers, the valves, etc If an intuitive model such as this could be studied, would we make advances in cardiovascular medicine and research? Using the object-oriented programming approach with the S4 class in R, it will be explored. "],
["using-s4.html", "5.1 Using S4", " 5.1 Using S4 # First S4 method library(methods) heart &lt;- setClass( &quot;heart&quot;, slots = c( plumbing = &quot;list&quot;, electric = &quot;list&quot;, structural = &quot;list&quot; ) ) # Make a new object first &lt;- new( &quot;heart&quot;, plumbing = list(&quot;LHC&quot;, &quot;RHC&quot;), electric = list(&quot;SA&quot;, &quot;AV&quot;), structural = list(&quot;LA&quot;, &quot;RA&quot;, &quot;LV&quot;, &quot;RV&quot;) ) # Or, through the generator function second &lt;- heart( plumbing = list(&quot;LHC&quot;, &quot;RHC&quot;), electric = list(&quot;SA&quot;, &quot;AV&quot;), structural = list(&quot;LA&quot;, &quot;RA&quot;, &quot;LV&quot;, &quot;RV&quot;) ) "],
["package.html", "Chapter 6 Package Development", " Chapter 6 Package Development CRAN supports the publishing of open-source packages in R. The workflow on package development is improved by the following supporting packages: devtools pkgdown usethis testthat roxygen hardhat (creating modeling functions) In addition, git and Github are fundamental for version control in the development process. These resources were used in the development of my first package, card. "],
["documenting-a-package.html", "6.1 Documenting a Package", " 6.1 Documenting a Package The use of roxygen is fundamental in the process of package development, forcing explanatory variables and parameters to be documented as functions are developed. 6.1.1 Website The package pkgdown helps turn documentation into a visually attractive and navigable website. In addition, in the fashion of R, package logos are developed with a hexagon-framed sticker, representing a package. The development of a hex sticker is aided by the use of: https://github.com/GuangchuangYu/hexSticker http://connect.thinkr.fr/hexmake/ "],
["building-a-modeling-package.html", "6.2 Building a Modeling Package", " 6.2 Building a Modeling Package With the development of the card::cosinor() function, the tidy approach was inspired by the work of Max Kuhn’s hardhat package. THe basic concepts are: models require a user-facing interface models require an internal, computational interface these two interfaces must be bridged 6.2.1 Blueprints The idea of bridging the user-interface and the computational interface seems to be the birth of the hardhat::new_blueprint() concept, with its derivatives for standard user-facing methods (formulas, recipes, matrices, etc). "],
["functions.html", "Chapter 7 Functions", " Chapter 7 Functions The current approach philosophically in R is that everything is either a function or an object. A function, essentially a “predicate”, is used on any object, essentially a “noun”. This workflow of output &lt;- function(input) builds on the concepts of functional programming, and is a good intellectual/philosophical approach to most problems. In R, a good function: allows for some level of flexibility in input with stability of output allows for back-ends to improve performance/functionality without impacting the function user is extensible "],
["speed-of-for-loops.html", "7.1 Speed of for loops", " 7.1 Speed of for loops Here is an issue with a function using for loops. Generally, if there is an iterative process internal to the function, it will have an \\(~O(N^2)\\) performance. The card::recur_survival_table is built with intuitive for loops, at a significant cost with increased N sizes. Here is the performance of the function: test replications elapsed relative user.self sys.self user.child sys.child 4 15000 1 901.169 618.086 882.081 18.467 0 0 3 5000 1 115.573 79.268 112.617 2.942 0 0 1 100 1 1.458 1.000 1.448 0.009 0 0 2 1000 1 15.268 10.472 15.234 0.036 0 0 ## [1] &quot;\\n test replications elapsed relative user.self sys.self user.child sys.child\\n4\\t\\t 15000\\t\\t\\t\\t\\t\\t1 901.169 618.086 882.081 18.467 0 0\\n3 \\t\\t5000 1 115.573 79.268 112.617 2.942 0 0\\n1 \\t\\t 100\\t 1 1.458 1.000 1.448 0.009 0 0\\n2 \\t\\t1000\\t\\t 1 15.268 10.472 15.234 0.036 0 0\\n&quot; The function currently relies on for loops througout, as seen below: print(card::recur_survival_table) ## function (data, id, first, last, event.dates, model.type, death = NULL) ## { ## if (is.null(death)) { ## df &lt;- data[c(id, first, last, event.dates)] ## death &lt;- &quot;null_death&quot; ## df$null_death &lt;- 0 ## } ## else { ## df &lt;- data[c(id, first, last, event.dates, death)] ## } ## n &lt;- 0:length(event.dates) ## events &lt;- paste0(&quot;EVENT_DATE_&quot;, c(1:(length(n) - 1))) ## x &lt;- df[c(id, event.dates)] %&gt;% tidyr::pivot_longer(-c(dplyr::all_of(id)), ## names_to = &quot;EVENT&quot;, values_to = &quot;DATE&quot;) %&gt;% dplyr::group_by_(dplyr::all_of(id)) %&gt;% ## dplyr::arrange(DATE) %&gt;% dplyr::arrange_(dplyr::all_of(id)) %&gt;% ## tidyr::nest() ## for (i in 1:length(x[[id]])) { ## if (!plyr::empty(x[[i, 2]][[1]][duplicated(x[[i, 2]][[1]]$DATE), ## ])) { ## x[[i, 2]][[1]][duplicated(x[[i, 2]][[1]]$DATE), ]$DATE &lt;- NA ## } ## x[[i, 2]][[1]] %&lt;&gt;% dplyr::arrange(DATE) ## x[[i, 2]][[1]]$EVENT &lt;- events ## } ## df &lt;- tidyr::unnest(x, cols = data) %&gt;% tidyr::pivot_wider(names_from = &quot;EVENT&quot;, ## values_from = &quot;DATE&quot;) %&gt;% dplyr::inner_join(df[c(id, ## first, last, death)], ., by = id) ## names(df) &lt;- c(&quot;ID&quot;, &quot;FIRST&quot;, &quot;LAST&quot;, &quot;DEATH&quot;, events) ## df$EVENT_DATE_0 &lt;- df$FIRST ## for (i in 1:length(n)) { ## df$EVENT_DATE_0[!is.na(df[paste0(&quot;EVENT_DATE_&quot;, n[i])])] &lt;- df[[paste0(&quot;EVENT_DATE_&quot;, ## n[i])]][!is.na(df[paste0(&quot;EVENT_DATE_&quot;, n[i])])] ## } ## x &lt;- df[c(&quot;ID&quot;, paste0(&quot;EVENT_DATE_&quot;, n))] %&gt;% tidyr::pivot_longer(-c(&quot;ID&quot;), ## names_to = &quot;EVENT&quot;, values_to = &quot;DATE&quot;) %&gt;% stats::na.omit() %&gt;% ## dplyr::left_join(df, ., by = &quot;ID&quot;) ## switch(model.type, marginal = { ## x$STATUS &lt;- x$TSTART &lt;- x$TSTOP &lt;- 0 ## for (i in 2:length(n)) { ## x$TSTOP[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] &lt;- x[[paste0(&quot;EVENT_DATE_&quot;, ## n[i])]][x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] - ## x$FIRST[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] ## } ## x$TSTOP[x$EVENT == &quot;EVENT_DATE_0&quot;] &lt;- x$LAST[x$EVENT == ## &quot;EVENT_DATE_0&quot;] - x$FIRST[x$EVENT == &quot;EVENT_DATE_0&quot;] ## x$STATUS[x$EVENT == &quot;EVENT_DATE_0&quot; &amp; x$DEATH == 1] &lt;- 1 ## x$STATUS[x$EVENT != &quot;EVENT_DATE_0&quot;] &lt;- 1 ## }, pwptt = { ## x$STATUS &lt;- x$TSTART &lt;- x$TSTOP &lt;- 0 ## x$TSTOP[x$EVENT == &quot;EVENT_DATE_0&quot;] &lt;- x$LAST[x$EVENT == ## &quot;EVENT_DATE_0&quot;] - x$FIRST[x$EVENT == &quot;EVENT_DATE_0&quot;] ## for (i in 2:length(n)) { ## x$TSTOP[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] &lt;- x[[paste0(&quot;EVENT_DATE_&quot;, ## n[i])]][x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] - ## x$FIRST[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] ## } ## x$TSTART[x$EVENT == &quot;EVENT_DATE_0&quot;] &lt;- x$EVENT_DATE_0[x$EVENT == ## &quot;EVENT_DATE_0&quot;] - x$FIRST[x$EVENT == &quot;EVENT_DATE_0&quot;] ## for (i in 2:length(n)) { ## if (n[i] == 1) { ## x$TSTART[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] &lt;- 0 ## } else { ## x$TSTART[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] &lt;- as.numeric(unlist(x[x$EVENT == ## paste0(&quot;EVENT_DATE_&quot;, n[i]), paste0(&quot;EVENT_DATE_&quot;, ## n[i - 1])] - x[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, ## n[i]), &quot;FIRST&quot;])) ## } ## } ## x$STATUS[x$EVENT == &quot;EVENT_DATE_0&quot; &amp; x$DEATH == 1] &lt;- 1 ## x$STATUS[x$EVENT != &quot;EVENT_DATE_0&quot;] &lt;- 1 ## }, pwpgt = { ## x$STATUS &lt;- x$TSTART &lt;- x$TSTOP &lt;- 0 ## x$TSTOP[x$EVENT == &quot;EVENT_DATE_0&quot;] &lt;- x$LAST[x$EVENT == ## &quot;EVENT_DATE_0&quot;] - x$FIRST[x$EVENT == &quot;EVENT_DATE_0&quot;] ## for (i in 2:length(n)) { ## x$TSTOP[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] &lt;- x[[paste0(&quot;EVENT_DATE_&quot;, ## n[i])]][x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] - ## x$FIRST[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] ## } ## x$TSTART[x$EVENT == &quot;EVENT_DATE_0&quot;] &lt;- x$EVENT_DATE_0[x$EVENT == ## &quot;EVENT_DATE_0&quot;] - x$FIRST[x$EVENT == &quot;EVENT_DATE_0&quot;] ## for (i in 2:length(n)) { ## if (n[i] == 1) { ## x$TSTART[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] &lt;- 0 ## } else { ## x$TSTART[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] &lt;- as.numeric(unlist(x[x$EVENT == ## paste0(&quot;EVENT_DATE_&quot;, n[i]), paste0(&quot;EVENT_DATE_&quot;, ## n[i - 1])] - x[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, ## n[i]), &quot;FIRST&quot;])) ## } ## } ## x$STATUS[x$EVENT == &quot;EVENT_DATE_0&quot; &amp; x$DEATH == 1] &lt;- 1 ## x$STATUS[x$EVENT != &quot;EVENT_DATE_0&quot;] &lt;- 1 ## x$TSTOP &lt;- x$TSTOP - x$TSTART ## x$TSTART &lt;- x$TSTART - x$TSTART ## }, stop(&quot;Need the correct repeat event model: marginal, pwptt, pwpgt&quot;)) ## y &lt;- x[c(&quot;ID&quot;, &quot;TSTART&quot;, &quot;TSTOP&quot;, &quot;STATUS&quot;, &quot;EVENT&quot;, &quot;DATE&quot;)] ## return(y) ## } ## &lt;bytecode: 0x7f8b88da59c0&gt; ## &lt;environment: namespace:card&gt; "],
["models.html", "Chapter 8 Modeling", " Chapter 8 Modeling Here is a collection of explorations on modeling approaches as they related to electrocardiography and epidemiology. "],
["modeling-multiple-outcomes-and-predictors.html", "8.1 Modeling Multiple Outcomes and Predictors", " 8.1 Modeling Multiple Outcomes and Predictors A recurrent issue with causality-focused modeling with ECG data is that there are multiple outcomes (different ECG features). For example, in the card package, the geh dataset contains several ECG features based on vectorcardiography. 8.1.1 Creating Multiple Models library(card) library(tidyverse) data(geh) names(geh) ## [1] &quot;pid&quot; &quot;hhp_id&quot; &quot;age&quot; ## [4] &quot;sex&quot; &quot;age_cat&quot; &quot;systolic_bp_first&quot; ## [7] &quot;systolic_bp_second&quot; &quot;systolic_bp_third&quot; &quot;diastolic_bp_first&quot; ## [10] &quot;diastolic_bp_second&quot; &quot;diastolic_bp_third&quot; &quot;pulse_rate_first&quot; ## [13] &quot;pulse_rate_second&quot; &quot;height_cm&quot; &quot;weight_kg&quot; ## [16] &quot;waist_cm&quot; &quot;dia_trt_allopdrug&quot; &quot;hbp_trt_allopdrug&quot; ## [19] &quot;hyp_trt_allopdrug&quot; &quot;lab_hba1c&quot; &quot;lab_fasting_bg&quot; ## [22] &quot;lab_fasting_insulin&quot; &quot;lab_tchol&quot; &quot;lab_ldlchol&quot; ## [25] &quot;lab_hdlchol&quot; &quot;lab_triglyc&quot; &quot;lab_ser_urea&quot; ## [28] &quot;lab_ser_creatinine&quot; &quot;lab_urin_malbumin&quot; &quot;pd_heart&quot; ## [31] &quot;bmi&quot; &quot;bmi_cat&quot; &quot;obese&quot; ## [34] &quot;obese_asian&quot; &quot;sbp_mean&quot; &quot;dbp_mean&quot; ## [37] &quot;pulse_mean&quot; &quot;htn&quot; &quot;cad&quot; ## [40] &quot;drugs_dm&quot; &quot;dm&quot; &quot;dm_lab&quot; ## [43] &quot;dm_control&quot; &quot;dm_pre&quot; &quot;homa&quot; ## [46] &quot;high_waist&quot; &quot;high_tchol&quot; &quot;high_ldl&quot; ## [49] &quot;low_hdl&quot; &quot;high_triglyc&quot; &quot;met_syn_num&quot; ## [52] &quot;met_syn&quot; &quot;pr_interval&quot; &quot;p_duration&quot; ## [55] &quot;p_amp&quot; &quot;qrs_duration&quot; &quot;qt_interval&quot; ## [58] &quot;cornell_voltage&quot; &quot;nhanes_score&quot; &quot;svg_mag&quot; ## [61] &quot;az_svg&quot; &quot;az_svg_m&quot; &quot;el_svg&quot; ## [64] &quot;el_svg_m&quot; &quot;qrs_tang&quot; &quot;auc_vm_qt&quot; ## [67] &quot;wvg&quot; &quot;log_svg&quot; &quot;log_auc_qt&quot; ## [70] &quot;log_wvg&quot; The first issue is the causal model, which can be visualized using a directed acyclic graph. The variables of interest are a subset of the dataset. In this case, we’re looking at the relationship of diabetes with cardiotoxicity in a very small subset of participants. library(ggdag) ## ## Attaching package: &#39;ggdag&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## filter bd &lt;- dagify( GEH ~ DM + Age + BMI + HTN + CAD + IR + Sex, DM ~ Age + IR + BMI, CAD ~ DM + BMI + HTN + Age + Sex, HTN ~ Age, IR ~ BMI + Age, Age ~ Sex, exposure = &quot;DM&quot;, outcome = &quot;GEH&quot; ) d1 &lt;- ggdag_parents(bd, &quot;DM&quot;, layout = &quot;star&quot;) + theme_dag() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Factors Affecting Diabetes&quot;) d2 &lt;- ggdag_parents(bd, &quot;GEH&quot;, layout = &quot;star&quot;) + theme_dag() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Factors Affecting GEH&quot;) # Combine and plot gridExtra::grid.arrange(d1, d2, nrow = 1) As we can see, many things effect ECG findings, and a subgroup of those impact diabetes, suggesting a number of potential effect modifiers and potential confounders/mediators. Using a sequential model building method in the card package allows for a simple way to perform this analysis. This will build a linear model for each outcome, and repeat the model with an additional covariate in the sequence of listed in the formula. # Select variables vars &lt;- c(&quot;svg_mag&quot;, &quot;az_svg&quot;, &quot;el_svg&quot;, &quot;qrs_tang&quot;, &quot;log_auc_qt&quot;, &quot;log_wvg&quot;, &quot;lab_hba1c&quot;, &quot;lab_fasting_bg&quot;, &quot;homa&quot;, &quot;dm&quot;, &quot;age&quot;, &quot;bmi&quot;, &quot;bmi_cat&quot;, &quot;age_cat&quot;, &quot;sex&quot;, &quot;htn&quot;, &quot;cad&quot;, &quot;lab_ser_creatinine&quot;, &quot;lab_tchol&quot;) df &lt;- geh %&gt;% select(all_of(vars)) %&gt;% #na.omit() %&gt;% #filter(homa &lt;= 5 * sd(homa, na.rm = TRUE)) %&gt;% # Remove outliers mutate( bmi_cat = factor(bmi_cat, levels = c(0:3), labels = c(&quot;Underweight&quot;, &quot;Normal&quot;, &quot;Overweight&quot;, &quot;Obese&quot;)), age_cat = factor(age_cat, levels = c(0:2), labels = c(&quot;&lt;45&quot;, &quot;45-65&quot;, &quot;&gt;65&quot;)), sex = factor(sex, levels = c(0,1), labels = c(&quot;Female&quot;, &quot;Male&quot;)) ) %&gt;% mutate(across( c(svg_mag, az_svg, el_svg, qrs_tang, log_auc_qt, log_wvg), function(x) { as.vector(scale(x, center = TRUE, scale = TRUE)) } )) # Sequential model building models &lt;- card::build_sequential_models( svg_mag + az_svg + el_svg + qrs_tang + log_auc_qt + log_wvg ~ lab_hba1c + age + sex + bmi + cad + htn, data = df, exposure = &quot;lab_hba1c&quot;, engine = &quot;lm&quot; ) head(models) ## # A tibble: 6 x 9 ## outcomes term estimate std.error statistic p.value conf.low conf.high covar ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 svg_mag (Inter… 0.339 0.196 1.73 0.0847 -0.0466 0.724 1 ## 2 svg_mag lab_hb… -0.0412 0.0234 -1.76 0.0796 -0.0872 0.00488 1 ## 3 az_svg (Inter… -0.315 0.195 -1.62 0.107 -0.698 0.0681 1 ## 4 az_svg lab_hb… 0.0381 0.0233 1.64 0.102 -0.00763 0.0839 1 ## 5 el_svg (Inter… 0.0767 0.195 0.394 0.694 -0.306 0.460 1 ## 6 el_svg lab_hb… -0.00862 0.0233 -0.370 0.711 -0.0544 0.0371 1 8.1.2 Visualize Regression Estimates To assess or get a sense of how the variables are playing out, we can visualize the estimates across the builds of the models. This will also use the gganimate package to show the effect of data layering # Libraries library(gganimate) library(ggthemes) # Data df &lt;- models %&gt;% # Remove intercepts filter(term != &quot;(Intercept)&quot;) %&gt;% # Sequence the terms mutate( term = factor( term, levels = c(&quot;lab_hba1c&quot;, &quot;age&quot;, &quot;sexMale&quot;, &quot;bmi&quot;, &quot;cad1&quot;, &quot;htn1&quot;), labels = c(&quot;HbA1c&quot;, &quot;Age&quot;, &quot;Sex&quot;, &quot;BMI&quot;, &quot;CAD&quot;, &quot;HTN&quot;) ) ) # ggplot g &lt;- ggplot(df, aes(x = factor(covar), y = estimate, color = term)) + facet_wrap(~outcomes, scales = &quot;fixed&quot;) + geom_point( aes(color = term), data = filter(df, p.value &gt;= 0.20), shape = 1, position = &quot;jitter&quot; ) + geom_point( aes(color = term), data = filter(df, p.value &lt; 0.20), shape = 19, position = &quot;jitter&quot; ) + scale_color_ptol(name = &quot;Predictors&quot;) + theme_minimal() + theme( legend.position = &quot;bottom&quot;, legend.box = &quot;horizontal&quot;, panel.border = element_rect(colour = &quot;black&quot;, fill = NA) ) + labs( title = &quot;Estimates in Sequential Models&quot;, x = &quot;Number of Covariates in Model&quot;, y = &quot;GEH Parameters (z-normalized)&quot; ) # Animated a &lt;- g + transition_reveal(covar) animate(a, end_pause = 30) "],
["vcg.html", "Chapter 9 Vectorcardiography ", " Chapter 9 Vectorcardiography "],
["vector-gradients.html", "9.1 Vector Gradients", " 9.1 Vector Gradients Mark Josephson in 1988 found that repolarization became non-uniform post-infarction, which was the suggestive substrate of VT/VF. The dispersion of the total recovery time is suggestive of global electrical heterogeneity, which can then predict SCD. Vectorcardiography (VCG) characterizes the electrical heart vector movement through a cardiac cycle. This is understood best through the spatial ventricular gradient vector (SVG), as described by Frank Wilson in 1934, and expanded upon by J. Willis Hurst. This is different and independent of the sequence of ventricular activation, which can be seen on ECG. The work by Larisa Tereschchenko has helped to compute these concepts for analytical approaches (Tereshchenko 2018). The SVG points to different locations in healthy versus diseased hearts. Summary of SVG: Points along the direction of greatest activation and recovery time (which is perpendicular to the line of conduction block, such as scar) Points towards to the area where the total recovery time is the shortest Depends on the heterogeneity of action potential across entire myocardium Characterizes the degree of heterogeneity of recovery time across the ventricles Steepness of the gradient determines magnitude of the SVG (areas of contrasting recovery time thus give largest SVG) References "],
["global-electrical-heterogeneity.html", "9.2 Global Electrical Heterogeneity", " 9.2 Global Electrical Heterogeneity The SVG can be broken down in 5 VCG parameters to describe the overall global electrical heterogeneity of the heart, as seen in the Figure below (Waks et al. 2016). SVG magnitude SVG azimuth SVG elevation Spatial QRS-T angle = the three-dimensional angle between mean spatial QRS-vector and mean spatial T-vector, measured in degrees Sum absolute QRST integral = scalar analog of the SVG, calculated as absolute value under QRS cmplex and T-wave, measured in millivolts (integral of voltage over time) vector gradient References "],
["cardiac-catheterization.html", "Chapter 10 Cardiac Catheterization", " Chapter 10 Cardiac Catheterization A large part of cardiovascular studies of interest are hemodynamic parameters. These parameters are specific to the structure of the heart (e.g. valves closing and opening, the heart contracting, etc), and cycle over a single wave. These patterns are seen as different waveforms in different chambers of the heart: RA = 1-5 mm Hg RV = 25/5 mm Hg PA = 20/10 mm Hg LA = 3-12 mm Hg LV = 120/12 mm Hg The waveforms could theoretically be recreated using an understanding of intracardiac hemodynamics (the “structure follows function” appraoch). As these are cyclic, sinusoidal curves would be appropriate for describing these curves. "],
["right-heart-catheterization.html", "10.1 Right heart catheterization", " 10.1 Right heart catheterization There are several chambers that are measured in a right-heart study. Usually, the order is: Right atrium pressure Right ventricle pressure Pulmonary artery pressure Pulmonary capillary wedge pressure The right atrial waveform is mapped onto several events: Waveform Cardiac Cycle Mechanical Process a end diastole atrial contraction c early systole tricuspid bulge v late systole systolic atrial filling x mid systole atrial relaxation y early diastole early ventricular filling Each of these can be mapped onto a time or phase-shift of a single cycle. For now, \\(2 pi\\) will be a cycle length. # Setup library(ggplot2) # Time positions of each peak and descent tau &lt;- 2*pi xa &lt;- 2/16 * tau xc &lt;- 8/16 * tau xx &lt;- 10/16 * tau xv &lt;- 12/16 * tau xy &lt;- 14/16 * tau # Height of peaks and troughs ya &lt;- 8 yc &lt;- 6 yx &lt;- 2 yv &lt;- 10 yy &lt;- 1 # Data library(mgcv) ## Loading required package: nlme ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse ## This is mgcv 1.8-31. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. df &lt;- data.frame(x = c(xa, xc, xx, xv, xy), y = c(ya, yc, yx, yv, yy)) m &lt;- gamm(y ~ s(x, bs = &quot;cc&quot;, k = 3), data = df) ## Warning in smooth.construct.cc.smooth.spec(object, dk$data, dk$knots): basis dimension, k, increased to minimum possible plot(m$gam) This simulates a portion of a single descent, but is problematic interms of cyclic nature and repeated measures. An alternative approach may be to consider segments of the data that are delayed based on findings from ECG (e.g. p-wave for atrial contraction). "],
["invasive-hemodynamics.html", "10.2 Invasive Hemodynamics", " 10.2 Invasive Hemodynamics During a cardiac catheterization, particularly of the right side, there are specific hemodynamics that can be assessed. The most important on right heart catheterization is cardiac output. 10.2.1 Fick’s cardiac output The Fick’s method for cardiac output relies on the concept that peripheral uptake of oxygen is equal to the blood flow to the periphery and the arterial-venous oxygen saturation gradient. There are several important concepts or parameters: oxygen consumption (V02) arterial oxygenation venous oxygenation How do we measure oxygenation? The blood oxygenation will be dependent on the amount of hemoglobin, how well it binds oxygen, the oxygen saturation, and the dissolved oxygen in the serum. Hemoglobin has an oxygen-binding capaciity of 1.34 mL/g. On average, an adult may have a hemoglobin level of 15 g/dL. The partial pressure of oxygen can also be measured, which is 21% of the atmospheric pressure at sea level (760 mmHg), thus approximately 160 mmHg. Thus, arterial (and similiarly venous) oxygenation can be calculated as: \\[CaO_{2} = (1.34 \\times Hb \\times SaO_{2}) + 0.0032 \\times PaO_{2}\\] At sea level, we can essentially exclude the contribution to partial pressure of oxygen. Thus, an average adult male would have a saturation of approximately 200 mL/dL. 10.2.2 Thermodilution Stewart-Hamilton equation. "],
["coronary-anatomy.html", "10.3 Coronary Anatomy", " 10.3 Coronary Anatomy The coronary anatomy tree is visualized during cardiac catheterization. Clinical reports label the epicardial vessels and their percent stenosis. 10.3.1 Text-processing An approach to analysis of LHC/RHC data is through text-processing, since cardiologists write very similar descriptions of procedures using a limited/specialized vocabulary. For example, we can set up the analysis of a single sentence from the findings section using a tidy approach. # Setup library(tidyverse) # From findings txt &lt;- &quot;Small caliber RCA with 50% proximal and 70% mid stenoses.&quot; # Convert to tidy format sentence &lt;- tibble(line = 1, sentence = txt) %&gt;% tidytext::unnest_tokens(input = sentence, output = word, to_lower = FALSE) %&gt;% pull(word) # Identify a big artery in sentence epicardial &lt;- c(&quot;LM&quot;, &quot;LAD&quot;, &quot;LCX&quot;, &quot;RCA&quot;) artery &lt;- sentence[which(sentence %in% epicardial)] # Identify number/locations of disease locs &lt;- grep(&quot;\\\\d+&quot;, sentence) # Modifier for disease location mods &lt;- c(&quot;proximal&quot;, &quot;mid&quot;, &quot;distal&quot;, &quot;ostial&quot;) mlocs &lt;- which(sentence %in% mods) # Find the nearest neighbors to identify which modifier goes with which location space &lt;- combn(mlocs, length(locs)) dist &lt;- apply(space, 2, function(x) {sum(abs(locs - x))}) matched &lt;- space[, which.min(dist)] # Now connect the artery with percent stenosis tibble( anatomy = paste(sentence[matched], artery), stenosis = as.numeric(sentence[locs]) ) %&gt;% mutate(description = glue::glue(&quot;The {anatomy} has {stenosis}% stenosis.&quot;)) ## # A tibble: 2 x 3 ## anatomy stenosis description ## &lt;chr&gt; &lt;dbl&gt; &lt;glue&gt; ## 1 proximal RCA 50 The proximal RCA has 50% stenosis. ## 2 mid RCA 70 The mid RCA has 70% stenosis. This method allows a computational approach at analyzing cath reports. This can be pushed into a function likely. # Function to turn sentence into tibble? coronary_anatomy &lt;- function(x) { # Check if sentence if(!is.character(x)) {stop(&quot;Requires character string&quot;, call. = FALSE)} # Establish variables epicardial &lt;- c(&quot;LM&quot;, &quot;LAD&quot;, &quot;LCX&quot;, &quot;RCA&quot;) mods &lt;- c(&quot;proximal&quot;, &quot;mid&quot;, &quot;distal&quot;, &quot;ostial&quot;) sentence &lt;- tibble(line = 1, sentence = x) %&gt;% tidytext::unnest_tokens(input = sentence, output = word, to_lower = FALSE) %&gt;% pull(word) # Identify number/locations of disease artery &lt;- sentence[which(sentence %in% epicardial)] locs &lt;- grep(&quot;\\\\d+&quot;, sentence) mlocs &lt;- which(sentence %in% mods) # Find the nearest neighbors to identify which modifier goes with which location space &lt;- combn(mlocs, length(locs)) dist &lt;- apply(space, 2, function(x) {sum(abs(locs - x))}) matched &lt;- space[, which.min(dist)] tbl &lt;- tibble( anatomy = paste(sentence[matched], artery), stenosis = as.numeric(sentence[locs]) ) # Return return(tbl) } # Test it out coronary_anatomy(txt) ## # A tibble: 2 x 2 ## anatomy stenosis ## &lt;chr&gt; &lt;dbl&gt; ## 1 proximal RCA 50 ## 2 mid RCA 70 "],
["echo.html", "Chapter 11 Echocardiography", " Chapter 11 Echocardiography Echocardiography is a tool used to analyze the function of the heart dynamically. It uses ultrasonography to evaluate the heart, and with the advent of doppler, can assess movement. A majority of these assessments of function are driven by mathematics/physics. "],
["chamber-size.html", "11.1 Chamber size", " 11.1 Chamber size The left ventricle can be dilated, and would thus have a change in shape/size. Because the density of myocardium is known, we can assess overall mass, e.g. how much the heart weighs. \\[LV\\ mass = 0.8 \\times 1.04 \\times [(IVS + LVID + PWT)^3 - LVID^3] + 0.6 \\] The tissue density of the myocardium is 1.04 mL/gm. The other measurements are available on echocardiography, but because it is cubed, small changes can dramatically change mass. The ejection fraction can also be calculated using similar approaches. Fractional shortening: \\[FS = (LVEDD - LVESD) / LVEDD\\] Quinones equation: \\[EF = (LVEDD^2 - LVESD^2) / LVEDD^2 + (apical\\ factor)\\] "],
["pressure-gradients.html", "11.2 Pressure gradients", " 11.2 Pressure gradients In echocardiography, relative velocity and known chamber sizes can help assess pressure of blood as it moves through the heart, which help to assess two important cardiac capacities: inotropy: the ability of the heart to contract lusitropy: the ability of the heart to relax "],
["tidymodels.html", "Chapter 12 Tidymodels", " Chapter 12 Tidymodels Expanding my understanding of the tidymodels approach. "],
["rmedicine-2020-and-rstudio-intro-course.html", "12.1 R/Medicine 2020 (and RStudio Intro Course)", " 12.1 R/Medicine 2020 (and RStudio Intro Course) As I begin to use tidymodels as a work-flow, I’m completing the #RMedicine2020 course. Initial setup below. 12.1.1 Introduction Mentions that statistics is focused on inference while machine learning is focused on prediction. library(tidyverse) library(tidymodels) ## Registered S3 methods overwritten by &#39;lme4&#39;: ## method from ## cooks.distance.influence.merMod car ## influence.merMod car ## dfbeta.influence.merMod car ## dfbetas.influence.merMod car ## ── Attaching packages ─────────────────── tidymodels 0.1.0 ── ## ✓ broom 0.7.0.9000 ✓ rsample 0.0.6 ## ✓ dials 0.0.6 ✓ tune 0.1.0 ## ✓ infer 0.5.1 ✓ workflows 0.1.1 ## ✓ parsnip 0.1.1 ✓ yardstick 0.0.6 ## ✓ recipes 0.1.12 ## ── Conflicts ────────────────────── tidymodels_conflicts() ── ## x nlme::collapse() masks dplyr::collapse() ## x scales::discard() masks purrr::discard() ## x ggdag::filter() masks dplyr::filter(), stats::filter() ## x recipes::fixed() masks stringr::fixed() ## x dplyr::lag() masks stats::lag() ## x dials::margin() masks ggplot2::margin() ## x yardstick::spec() masks readr::spec() ## x recipes::step() masks stats::step() 12.1.2 Build a Model Alzheimer’s disease data. library(modeldata) ## Warning: package &#39;modeldata&#39; was built under R version 4.0.2 data(&quot;ad_data&quot;) alz &lt;- ad_data glimpse(alz) ## Rows: 333 ## Columns: 131 ## $ ACE_CD143_Angiotensin_Converti &lt;dbl&gt; 2.0031003, 1.5618560, 1.5206598, 1.6… ## $ ACTH_Adrenocorticotropic_Hormon &lt;dbl&gt; -1.3862944, -1.3862944, -1.7147984, … ## $ AXL &lt;dbl&gt; 1.09838668, 0.68328157, -0.14527630,… ## $ Adiponectin &lt;dbl&gt; -5.360193, -5.020686, -5.809143, -5.… ## $ Alpha_1_Antichymotrypsin &lt;dbl&gt; 1.7404662, 1.4586150, 1.1939225, 1.2… ## $ Alpha_1_Antitrypsin &lt;dbl&gt; -12.631361, -11.909882, -13.642963, … ## $ Alpha_1_Microglobulin &lt;dbl&gt; -2.577022, -3.244194, -2.882404, -3.… ## $ Alpha_2_Macroglobulin &lt;dbl&gt; -72.65029, -154.61228, -136.52918, -… ## $ Angiopoietin_2_ANG_2 &lt;dbl&gt; 1.06471074, 0.74193734, 0.83290912, … ## $ Angiotensinogen &lt;dbl&gt; 2.510547, 2.457283, 1.976365, 2.3760… ## $ Apolipoprotein_A_IV &lt;dbl&gt; -1.427116, -1.660731, -1.660731, -2.… ## $ Apolipoprotein_A1 &lt;dbl&gt; -7.402052, -7.047017, -7.684284, -8.… ## $ Apolipoprotein_A2 &lt;dbl&gt; -0.26136476, -0.86750057, -0.6539264… ## $ Apolipoprotein_B &lt;dbl&gt; -4.624044, -6.747507, -3.976069, -6.… ## $ Apolipoprotein_CI &lt;dbl&gt; -1.2729657, -1.2729657, -1.7147984, … ## $ Apolipoprotein_CIII &lt;dbl&gt; -2.312635, -2.343407, -2.748872, -2.… ## $ Apolipoprotein_D &lt;dbl&gt; 2.0794415, 1.3350011, 1.3350011, 1.4… ## $ Apolipoprotein_E &lt;dbl&gt; 3.7545215, 3.0971187, 2.7530556, 2.3… ## $ Apolipoprotein_H &lt;dbl&gt; -0.15734908, -0.57539617, -0.3448393… ## $ B_Lymphocyte_Chemoattractant_BL &lt;dbl&gt; 2.2969819, 1.6731213, 1.6731213, 1.9… ## $ BMP_6 &lt;dbl&gt; -2.200744, -1.728053, -2.062421, -1.… ## $ Beta_2_Microglobulin &lt;dbl&gt; 0.69314718, 0.47000363, 0.33647224, … ## $ Betacellulin &lt;int&gt; 34, 53, 49, 52, 67, 51, 41, 42, 58, … ## $ C_Reactive_Protein &lt;dbl&gt; -4.074542, -6.645391, -8.047190, -6.… ## $ CD40 &lt;dbl&gt; -0.7964147, -1.2733760, -1.2415199, … ## $ CD5L &lt;dbl&gt; 0.09531018, -0.67334455, 0.09531018,… ## $ Calbindin &lt;dbl&gt; 33.21363, 25.27636, 22.16609, 23.455… ## $ Calcitonin &lt;dbl&gt; 1.3862944, 3.6109179, 2.1162555, -0.… ## $ CgA &lt;dbl&gt; 397.6536, 465.6759, 347.8639, 334.23… ## $ Clusterin_Apo_J &lt;dbl&gt; 3.555348, 3.044522, 2.772589, 2.8332… ## $ Complement_3 &lt;dbl&gt; -10.36305, -16.10824, -16.10824, -13… ## $ Complement_Factor_H &lt;dbl&gt; 3.5737252, 3.6000471, 4.4745686, 3.0… ## $ Connective_Tissue_Growth_Factor &lt;dbl&gt; 0.5306283, 0.5877867, 0.6418539, 0.5… ## $ Cortisol &lt;dbl&gt; 10.0, 12.0, 10.0, 14.0, 11.0, 13.0, … ## $ Creatine_Kinase_MB &lt;dbl&gt; -1.710172, -1.751002, -1.383559, -1.… ## $ Cystatin_C &lt;dbl&gt; 9.041922, 9.067624, 8.954157, 9.5819… ## $ EGF_R &lt;dbl&gt; -0.1354543, -0.3700474, -0.7329871, … ## $ EN_RAGE &lt;dbl&gt; -3.688879, -3.816713, -4.755993, -2.… ## $ ENA_78 &lt;dbl&gt; -1.349543, -1.356595, -1.390672, -1.… ## $ Eotaxin_3 &lt;int&gt; 53, 62, 62, 44, 64, 57, 64, 64, 64, … ## $ FAS &lt;dbl&gt; -0.08338161, -0.52763274, -0.6348782… ## $ FSH_Follicle_Stimulation_Hormon &lt;dbl&gt; -0.6516715, -1.6272839, -1.5630004, … ## $ Fas_Ligand &lt;dbl&gt; 3.1014922, 2.9788133, 1.3600098, 2.5… ## $ Fatty_Acid_Binding_Protein &lt;dbl&gt; 2.5208712, 2.2477966, 0.9063009, 0.6… ## $ Ferritin &lt;dbl&gt; 3.329165, 3.932959, 3.176872, 3.1380… ## $ Fetuin_A &lt;dbl&gt; 1.2809338, 1.1939225, 1.4109870, 0.7… ## $ Fibrinogen &lt;dbl&gt; -7.035589, -8.047190, -7.195437, -7.… ## $ GRO_alpha &lt;dbl&gt; 1.381830, 1.372438, 1.412679, 1.3724… ## $ Gamma_Interferon_induced_Monokin &lt;dbl&gt; 2.949822, 2.721793, 2.762231, 2.8854… ## $ Glutathione_S_Transferase_alpha &lt;dbl&gt; 1.0641271, 0.8670202, 0.8890150, 0.7… ## $ HB_EGF &lt;dbl&gt; 6.559746, 8.754531, 7.745463, 5.9494… ## $ HCC_4 &lt;dbl&gt; -3.036554, -4.074542, -3.649659, -3.… ## $ Hepatocyte_Growth_Factor_HGF &lt;dbl&gt; 0.58778666, 0.53062825, 0.09531018, … ## $ I_309 &lt;dbl&gt; 3.433987, 3.135494, 2.397895, 3.3672… ## $ ICAM_1 &lt;dbl&gt; -0.1907787, -0.4620172, -0.4620172, … ## $ IGF_BP_2 &lt;dbl&gt; 5.609472, 5.347108, 5.181784, 5.4249… ## $ IL_11 &lt;dbl&gt; 5.121987, 4.936704, 4.665910, 6.2239… ## $ IL_13 &lt;dbl&gt; 1.282549, 1.269463, 1.274133, 1.3075… ## $ IL_16 &lt;dbl&gt; 4.192081, 2.876338, 2.616102, 2.4410… ## $ IL_17E &lt;dbl&gt; 5.731246, 6.705891, 4.149327, 4.6958… ## $ IL_1alpha &lt;dbl&gt; -6.571283, -8.047190, -8.180721, -7.… ## $ IL_3 &lt;dbl&gt; -3.244194, -3.912023, -4.645992, -4.… ## $ IL_4 &lt;dbl&gt; 2.484907, 2.397895, 1.824549, 1.4816… ## $ IL_5 &lt;dbl&gt; 1.09861229, 0.69314718, -0.24846136,… ## $ IL_6 &lt;dbl&gt; 0.26936976, 0.09622438, 0.18568645, … ## $ IL_6_Receptor &lt;dbl&gt; 0.64279595, 0.43115645, 0.09668586, … ## $ IL_7 &lt;dbl&gt; 4.8050453, 3.7055056, 1.0056222, 2.3… ## $ IL_8 &lt;dbl&gt; 1.711325, 1.675557, 1.691393, 1.7199… ## $ IP_10_Inducible_Protein_10 &lt;dbl&gt; 6.242223, 5.686975, 5.049856, 5.6021… ## $ IgA &lt;dbl&gt; -6.812445, -6.377127, -6.319969, -7.… ## $ Insulin &lt;dbl&gt; -0.6258253, -0.9431406, -1.4466191, … ## $ Kidney_Injury_Molecule_1_KIM_1 &lt;dbl&gt; -1.204295, -1.197703, -1.191191, -1.… ## $ LOX_1 &lt;dbl&gt; 1.7047481, 1.5260563, 1.1631508, 1.2… ## $ Leptin &lt;dbl&gt; -1.5290628, -1.4660558, -1.6622675, … ## $ Lipoprotein_a &lt;dbl&gt; -4.268698, -4.933674, -5.843045, -4.… ## $ MCP_1 &lt;dbl&gt; 6.740519, 6.849066, 6.767343, 6.7810… ## $ MCP_2 &lt;dbl&gt; 1.9805094, 1.8088944, 0.4005958, 1.9… ## $ MIF &lt;dbl&gt; -1.237874, -1.897120, -2.302585, -1.… ## $ MIP_1alpha &lt;dbl&gt; 4.968453, 3.690160, 4.049508, 4.9285… ## $ MIP_1beta &lt;dbl&gt; 3.258097, 3.135494, 2.397895, 3.2188… ## $ MMP_2 &lt;dbl&gt; 4.478566, 3.781473, 2.866631, 2.9685… ## $ MMP_3 &lt;dbl&gt; -2.207275, -2.465104, -2.302585, -1.… ## $ MMP10 &lt;dbl&gt; -3.270169, -3.649659, -2.733368, -4.… ## $ MMP7 &lt;dbl&gt; -3.7735027, -5.9681907, -4.0302269, … ## $ Myoglobin &lt;dbl&gt; -1.89711998, -0.75502258, -1.3862943… ## $ NT_proBNP &lt;dbl&gt; 4.553877, 4.219508, 4.248495, 4.1108… ## $ NrCAM &lt;dbl&gt; 5.003946, 5.209486, 4.744932, 4.9698… ## $ Osteopontin &lt;dbl&gt; 5.356586, 6.003887, 5.017280, 5.7683… ## $ PAI_1 &lt;dbl&gt; 1.00350156, -0.03059880, 0.43837211,… ## $ PAPP_A &lt;dbl&gt; -2.902226, -2.813276, -2.935541, -2.… ## $ PLGF &lt;dbl&gt; 4.442651, 4.025352, 4.510860, 3.4339… ## $ PYY &lt;dbl&gt; 3.218876, 3.135494, 2.890372, 2.8332… ## $ Pancreatic_polypeptide &lt;dbl&gt; 0.57878085, 0.33647224, -0.89159812,… ## $ Prolactin &lt;dbl&gt; 0.00000000, -0.51082562, -0.13926207… ## $ Prostatic_Acid_Phosphatase &lt;dbl&gt; -1.620527, -1.739232, -1.636682, -1.… ## $ Protein_S &lt;dbl&gt; -1.784998, -2.463991, -2.259135, -2.… ## $ Pulmonary_and_Activation_Regulat &lt;dbl&gt; -0.8439701, -2.3025851, -1.6607312, … ## $ RANTES &lt;dbl&gt; -6.214608, -6.938214, -6.645391, -5.… ## $ Resistin &lt;dbl&gt; -16.475315, -16.025283, -16.475315, … ## $ S100b &lt;dbl&gt; 1.5618560, 1.7566212, 1.4357282, 1.2… ## $ SGOT &lt;dbl&gt; -0.94160854, -0.65392647, 0.33647224… ## $ SHBG &lt;dbl&gt; -1.897120, -1.560648, -2.207275, -3.… ## $ SOD &lt;dbl&gt; 5.609472, 5.814131, 5.723585, 5.7714… ## $ Serum_Amyloid_P &lt;dbl&gt; -5.599422, -6.119298, -5.381699, -6.… ## $ Sortilin &lt;dbl&gt; 4.908629, 5.478731, 3.810182, 3.4021… ## $ Stem_Cell_Factor &lt;dbl&gt; 4.174387, 3.713572, 3.433987, 3.9512… ## $ TGF_alpha &lt;dbl&gt; 8.649098, 11.331619, 10.858497, 9.45… ## $ TIMP_1 &lt;dbl&gt; 15.204651, 11.266499, 12.282857, 11.… ## $ TNF_RII &lt;dbl&gt; -0.06187540, -0.32850407, -0.4155154… ## $ TRAIL_R3 &lt;dbl&gt; -0.1829004, -0.5007471, -0.9240345, … ## $ TTR_prealbumin &lt;dbl&gt; 2.944439, 2.833213, 2.944439, 2.9444… ## $ Tamm_Horsfall_Protein_THP &lt;dbl&gt; -3.095810, -3.111190, -3.166721, -3.… ## $ Thrombomodulin &lt;dbl&gt; -1.340566, -1.675252, -1.534276, -1.… ## $ Thrombopoietin &lt;dbl&gt; -0.1026334, -0.6733501, -0.9229670, … ## $ Thymus_Expressed_Chemokine_TECK &lt;dbl&gt; 4.149327, 3.810182, 2.791992, 4.0372… ## $ Thyroid_Stimulating_Hormone &lt;dbl&gt; -3.863233, -4.828314, -4.990833, -4.… ## $ Thyroxine_Binding_Globulin &lt;dbl&gt; -1.4271164, -1.6094379, -1.8971200, … ## $ Tissue_Factor &lt;dbl&gt; 2.04122033, 2.02814825, 1.43508453, … ## $ Transferrin &lt;dbl&gt; 3.332205, 2.890372, 2.890372, 2.8903… ## $ Trefoil_Factor_3_TFF3 &lt;dbl&gt; -3.381395, -3.912023, -3.729701, -3.… ## $ VCAM_1 &lt;dbl&gt; 3.258097, 2.708050, 2.639057, 2.7725… ## $ VEGF &lt;dbl&gt; 22.03456, 18.60184, 17.47619, 17.545… ## $ Vitronectin &lt;dbl&gt; -0.04082199, -0.38566248, -0.2231435… ## $ von_Willebrand_Factor &lt;dbl&gt; -3.146555, -3.863233, -3.540459, -3.… ## $ age &lt;dbl&gt; 0.9876238, 0.9861496, 0.9866667, 0.9… ## $ tau &lt;dbl&gt; 6.297754, 6.659294, 6.270988, 6.1527… ## $ p_tau &lt;dbl&gt; 4.348108, 4.859967, 4.400247, 4.4948… ## $ Ab_42 &lt;dbl&gt; 12.019678, 11.015759, 12.302271, 12.… ## $ male &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, … ## $ Genotype &lt;fct&gt; E3E3, E3E4, E3E4, E3E4, E3E3, E4E4, … ## $ Class &lt;fct&gt; Control, Control, Control, Control, … This data has 333, with 1 categorical outcome, and 130 predictors (126 protein measurements) and age, sex, and genetics. Models can be built using parsnip package, which is different than base R (seen below) glm(Class ~ tau, family = binomial, data = alz) ## ## Call: glm(formula = Class ~ tau, family = binomial, data = alz) ## ## Coefficients: ## (Intercept) tau ## 13.664 -2.148 ## ## Degrees of Freedom: 332 Total (i.e. Null); 331 Residual ## Null Deviance: 390.6 ## Residual Deviance: 318.8 AIC: 322.8 For parsnip, there are three general steps: Pick a model Set the engine Set the mode (if needed) # Using logistic reggresion as the model. logistic_reg( mode = &quot;classification&quot;, # default if exists penalty = NULL, # hyperparameter mixture = NULL # hyperparameter ) ## Logistic Regression Model Specification (classification) # Setting the engine to power/implement hte model logistic_reg() %&gt;% set_engine(engine = &quot;glm&quot;) ## Logistic Regression Model Specification (classification) ## ## Computational engine: glm # Can add a mod eif not set prior lr_mod &lt;- logistic_reg() %&gt;% set_engine(engine = &quot;glm&quot;) %&gt;% set_mode(mode = &quot;classification&quot;) As an example, make a decision tree model for classification, using C5.0 engine, and saved as tree_mod to evaluate. # Decision tree tree_mod &lt;- decision_tree() %&gt;% set_engine(&quot;C5.0&quot;) %&gt;% set_mode(&quot;classification&quot;) tree_mod ## Decision Tree Model Specification (classification) ## ## Computational engine: C5.0 summary(tree_mod) ## Length Class Mode ## args 3 -none- list ## eng_args 0 quosures list ## mode 1 -none- character ## method 0 -none- NULL ## engine 1 -none- character Fitting a model… lr_mod %&gt;% fit(Class ~ tau + VEGF, data = alz) %&gt;% broom::tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 8.97 1.98 4.54 5.61e- 6 ## 2 tau -4.01 0.456 -8.79 1.55e-18 ## 3 VEGF 0.934 0.130 7.19 6.38e-13 Predicting data… # Generating new data alz_new &lt;- tibble( tau = c(5:7), VEGF = rep(15, 3), Class = c(&quot;Control&quot;, &quot;Control&quot;, &quot;Impaired&quot;) ) %&gt;% mutate(Class = factor(Class, levels = c(&quot;Impaired&quot;, &quot;Control&quot;))) alz_new ## # A tibble: 3 x 3 ## tau VEGF Class ## &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5 15 Control ## 2 6 15 Control ## 3 7 15 Impaired # Predicting old data tree_mod %&gt;% fit(Class ~ tau + VEGF, data = alz) %&gt;% predict(new_data = alz) %&gt;% mutate(true_class = alz$Class) %&gt;% accuracy(truth = true_class, estimate = .pred_class) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.856 # Creating new data with old predictions tree_mod %&gt;% fit(Class ~ tau + VEGF, data = alz) %&gt;% predict(new_data = alz_new) %&gt;% mutate(true_class = alz_new$Class) %&gt;% accuracy(truth = true_class, estimate = .pred_class) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.667 Fitting a model can be done in pipeline fashion, which can then be used to predict outcomes. Axiom: best way to measure model performance at predicting new data is to predict new data # Fitting a model fit(tree_mod, Class ~ tau + VEGF, data = alz) ## parsnip model object ## ## Fit time: 6ms ## ## Call: ## C5.0.default(x = x, y = y, trials = 1, control = C50::C5.0Control(minCases = ## 2, sample = 0)) ## ## Classification Tree ## Number of samples: 333 ## Number of predictors: 2 ## ## Tree size: 4 ## ## Non-standard options: attempt to group attributes # Step by step tree_fit &lt;- tree_mod %&gt;% # parsnip model fit( Class ~ tau + VEGF, # Formula data = alz # dataframe ) # Predictions. tree_fit %&gt;% predict(new_data = alz_new) ## # A tibble: 3 x 1 ## .pred_class ## &lt;fct&gt; ## 1 Control ## 2 Impaired ## 3 Impaired To test out data, splitting and sampling the data is needed. rsample package allows for this. # Split data into single testing and training set alz_split &lt;- initial_split(alz, strata = Class, prop = 0.9) # Training and testing the data splits can be extracted ussing rsample alz_train &lt;- training(alz_split) alz_test &lt;- testing(alz_split) As an example, using the alz data, can use test and train datasets to fit a classification tree model. Predict the test data and save the true classification ability by measuring accuracy. # Train the data tree_fit &lt;- tree_mod %&gt;% fit( Class ~ tau + VEGF, data = alz_train ) # Test by predicting tree_fit %&gt;% predict(new_data = alz_test) ## # A tibble: 33 x 1 ## .pred_class ## &lt;fct&gt; ## 1 Impaired ## 2 Impaired ## 3 Control ## 4 Impaired ## 5 Control ## 6 Impaired ## 7 Control ## 8 Control ## 9 Impaired ## 10 Impaired ## # … with 23 more rows tree_mod %&gt;% fit(Class ~ tau + VEGF, data = alz) %&gt;% predict(new_data = alz_new) %&gt;% mutate(true_class = alz_new$Class) %&gt;% accuracy(truth = true_class, estimate = .pred_class) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.667 # Using decision tree model tree_mod %&gt;% fit(Class ~ tau + VEGF, data = alz_train) %&gt;% predict(new_data = alz_test) %&gt;% mutate(true_class = alz_test$Class) %&gt;% accuracy(truth = true_class, estimate = .pred_class) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.788 Measuring the model is through the accuracy() function, which is based on teh two columns of a dataframe… the truth: \\(y_{i}\\) the predicted estimate: \\(\\hat{y}_{i}\\) This accuracy is dependent on the training/test sets, which is dependent on the overall split (which is random). Resampling is important to fix this problem. # Non-tidy method. Very slow... acc &lt;- vector(length = 10, mode = &quot;double&quot;) for(i in 1:10) { new_split &lt;- initial_split(alz) new_train &lt;- training(new_split) new_test &lt;- testing(new_split) acc[i] &lt;- lr_mod %&gt;% fit(Class ~ tau + VEGF, data = new_train) %&gt;% predict(new_test) %&gt;% mutate(truth = new_test$Class) %&gt;% accuracy(truth, .pred_class) %&gt;% pull(.estimate) } # Cross validation works better and is faster # save the testing data for the end, don&#39;t waste it on training alz_folds &lt;- vfold_cv(alz_train, v = 10, strata = Class) alz_folds %&gt;% pluck(&quot;splits&quot;, 1) ## &lt;Training/Validation/Total&gt; ## &lt;269/31/300&gt; # Using multiple fits is easier tree_mod %&gt;% fit_resamples( Class ~ tau + VEGF, resamples = alz_folds ) %&gt;% collect_metrics(summarize = TRUE) ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 accuracy binary 0.830 10 0.00823 ## 2 roc_auc binary 0.794 10 0.0213 12.1.3 Build a Workflow Recipes in R allow for a alternative way to create and preprocess the design matrix needed for modeling. encode categorical predictors center/scale variables handle class imbalance impute missingness perform dimensionality reduction etc… General workflow is data is processed with a recipe to create a model. Those are prepared and trained to allow new data to be made on a trained model, which can then allow predictions. These predictions can be compared with the original model, and improved. Building a recipe is several steps… Start the recipe() Define the variables involved Describe the pre-processing step-by-step # Recipe with steps rec &lt;- recipe(Class ~ ., data = alz) %&gt;% # recipe step_other(Genotype, threshold = 0.03) # Steps In this data set, will use the KNN approach to predict new data points. KNN needs predictors to be numeric and centered/scaled. # Specify parsnip model to make KNN knn_mod &lt;- nearest_neighbor() %&gt;% set_engine(&quot;kknn&quot;) %&gt;% set_mode(&quot;classification&quot;) # Recipe knn_rec &lt;- recipe(Class ~ ., data = alz) %&gt;% step_other(Genotype, threshold = 0.03) %&gt;% step_novel(all_nominal(), -all_outcomes()) %&gt;% # In case new data has levels that have missing data from original dataset step_dummy(all_nominal(), -all_outcomes()) %&gt;% # Make dummies for nominals step_zv(all_predictors()) %&gt;% step_normalize(all_numeric()) To use this recipe, we need to use workflow. # Without pre-processing workflow() %&gt;% add_formula(Class ~ tau) ## ══ Workflow ═════════════════════════════════════════════════ ## Preprocessor: Formula ## Model: None ## ## ── Preprocessor ───────────────────────────────────────────── ## Class ~ tau # If recipe is already made... workflow() %&gt;% add_recipe(knn_rec) %&gt;% add_model(knn_mod) %&gt;% fit(data = alz_train) %&gt;% predict(alz_test) ## # A tibble: 33 x 1 ## .pred_class ## &lt;fct&gt; ## 1 Control ## 2 Impaired ## 3 Control ## 4 Impaired ## 5 Control ## 6 Impaired ## 7 Impaired ## 8 Control ## 9 Impaired ## 10 Impaired ## # … with 23 more rows # Or can use k-folds if needed alz_folds &lt;- vfold_cv(alz_train, v = 10, strata = Class) workflow() %&gt;% add_recipe(knn_rec) %&gt;% add_model(knn_mod) %&gt;% fit_resamples(resamples = alz_folds) %&gt;% collect_metrics() ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 accuracy binary 0.710 10 0.0201 ## 2 roc_auc binary 0.776 10 0.0203 # Because workflows allow modifications, can change model quite easily using old recipe plr_mod &lt;- logistic_reg(penalty = 0.1, mixture = 1) %&gt;% set_engine(&quot;glmnet&quot;) %&gt;% set_mode(&quot;classification&quot;) plr_mod %&gt;% translate() ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = 0.1 ## mixture = 1 ## ## Computational engine: glmnet ## ## Model fit template: ## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), ## alpha = 1, family = &quot;binomial&quot;) # Workflow reuse for glmnet knn_wf &lt;- workflow() %&gt;% add_recipe(knn_rec) %&gt;% add_model(knn_mod) glmnet_wf &lt;- knn_wf %&gt;% update_model(plr_mod) glmnet_wf %&gt;% fit_resamples(resamples = alz_folds) %&gt;% collect_metrics() ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 accuracy binary 0.760 10 0.0159 ## 2 roc_auc binary 0.829 10 0.0316 12.1.4 Tune Better Models In decision tree models, predicting outcome of a new data point uses rules learned from split, and each split maximizes information gain. # Specify decision tree in parsnip tree_mod &lt;- decision_tree() %&gt;% set_engine(engine = &quot;rpart&quot;) %&gt;% set_mode(&quot;classification&quot;) # Add workflow tree_wf &lt;- workflow() %&gt;% add_formula(Class ~ .) %&gt;% add_model(tree_mod) # Get AUC tree_wf %&gt;% fit_resamples(resamples = alz_folds) %&gt;% collect_metrics() ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 accuracy binary 0.783 10 0.0292 ## 2 roc_auc binary 0.770 10 0.0368 # However, decision trees can be more complex # See arguments for a decision tree args(decision_tree) ## function (mode = &quot;unknown&quot;, cost_complexity = NULL, tree_depth = NULL, ## min_n = NULL) ## NULL decision_tree( tree_depth = 30, # Max depth can be used to stop overfitting min_n = 20, # Smallest node allowed (too few leads to overfitting) cost_complexity = .01 # 0 &gt; cp &gt; 0.1, 0 = large tree, 1 = smaller tree ) ## Decision Tree Model Specification (unknown) ## ## Main Arguments: ## cost_complexity = 0.01 ## tree_depth = 30 ## min_n = 20 # Can update the arguments for decision tree decision_tree() %&gt;% set_engine(&quot;rpart&quot;) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_args(tree_depth = 3) ## Decision Tree Model Specification (classification) ## ## Main Arguments: ## tree_depth = 3 ## ## Computational engine: rpart As an example, create a model rf_mod which uses classification trees from the ranger package. Use a 10-fold cross-validation and compare the AUC. # Instead of decision trees, can use a random forest rand_forest( mtry = 4, # predictors seen at each node trees = 500, # trees per forest min_n = 1 # smallest node allowed ) ## Random Forest Model Specification (unknown) ## ## Main Arguments: ## mtry = 4 ## trees = 500 ## min_n = 1 rf_mod &lt;- rand_forest() %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_mode(&quot;classification&quot;) # Random forest workflow rf_wf &lt;- tree_wf %&gt;% update_model(rf_mod) # Fit using 10-fold CV rf_wf %&gt;% fit_resamples(resamples = alz_folds) %&gt;% collect_metrics() ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 accuracy binary 0.823 10 0.0134 ## 2 roc_auc binary 0.888 10 0.0173 # Change number of trys to 3, 8, and 30 rf3 &lt;- rf_mod %&gt;% set_args(mtry = 3) rf8 &lt;- rf_mod %&gt;% set_args(mtry = 8) rf30 &lt;- rf_mod %&gt;% set_args(mtry = 30) rf_wf %&gt;% update_model(rf3) %&gt;% fit_resamples(resamples = alz_folds) %&gt;% collect_metrics() ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 accuracy binary 0.787 10 0.0121 ## 2 roc_auc binary 0.859 10 0.0189 rf_wf %&gt;% update_model(rf8) %&gt;% fit_resamples(resamples = alz_folds) %&gt;% collect_metrics() ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 accuracy binary 0.813 10 0.0119 ## 2 roc_auc binary 0.878 10 0.0165 rf_wf %&gt;% update_model(rf30) %&gt;% fit_resamples(resamples = alz_folds) %&gt;% collect_metrics() ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 accuracy binary 0.853 10 0.0194 ## 2 roc_auc binary 0.897 10 0.0173 Adjusting these parameters is consider tuning. We can perform a grid search to find the best combination of tuned hyperparameters using tune_grid(). # Create tuning parameters rf_tuner &lt;- rand_forest( mtry = tune(), min_n = tune() ) %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_mode(&quot;classification&quot;) rf_wf &lt;- rf_wf %&gt;% update_model(rf_tuner) rf_results &lt;- rf_wf %&gt;% tune_grid( resamples = alz_folds, metrics = metric_set(roc_auc) ) ## i Creating pre-processing data to finalize unknown parameter: mtry # Results of tune grid rf_results %&gt;% collect_metrics() ## # A tibble: 10 x 7 ## mtry min_n .metric .estimator mean n std_err ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 10 12 roc_auc binary 0.881 10 0.0172 ## 2 27 17 roc_auc binary 0.891 10 0.0169 ## 3 36 37 roc_auc binary 0.897 10 0.0205 ## 4 51 2 roc_auc binary 0.896 10 0.0207 ## 5 56 25 roc_auc binary 0.893 10 0.0198 ## 6 81 35 roc_auc binary 0.892 10 0.0222 ## 7 94 32 roc_auc binary 0.889 10 0.0216 ## 8 107 7 roc_auc binary 0.885 10 0.0188 ## 9 117 21 roc_auc binary 0.889 10 0.0204 ## 10 124 19 roc_auc binary 0.885 10 0.0200 rf_results %&gt;% show_best(metric = &quot;roc_auc&quot;, n = 5) ## # A tibble: 5 x 7 ## mtry min_n .metric .estimator mean n std_err ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 36 37 roc_auc binary 0.897 10 0.0205 ## 2 51 2 roc_auc binary 0.896 10 0.0207 ## 3 56 25 roc_auc binary 0.893 10 0.0198 ## 4 81 35 roc_auc binary 0.892 10 0.0222 ## 5 27 17 roc_auc binary 0.891 10 0.0169 rf_results %&gt;% autoplot() alz_best &lt;- rf_results %&gt;% select_best(metric = &quot;roc_auc&quot;) # Finalize with best workflow last_rf_workflow &lt;- rf_wf %&gt;% finalize_workflow(alz_best) # Last fit last_rf_fit &lt;- last_rf_workflow %&gt;% last_fit(split = alz_split) last_rf_fit %&gt;% collect_metrics() ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.848 ## 2 roc_auc binary 0.912 roc_values &lt;- last_rf_fit %&gt;% collect_predictions() %&gt;% roc_curve(truth = Class, estimate = .pred_Impaired) autoplot(roc_values) "]
]
