<<<<<<< HEAD
[["models.html", "Chapter 8 Modeling", " Chapter 8 Modeling Here is a collection of explorations on modeling approaches as they related to electrocardiography and epidemiology. This is a sandbox for developing meta-modeling functions as well. Multiple models Visualizing multiple models New functions to build models in certain sequences Integration for parsnip models "],["modeling-multiple-outcomes-and-predictors.html", "8.1 Modeling Multiple Outcomes and Predictors", " 8.1 Modeling Multiple Outcomes and Predictors A recurrent issue with causality-focused modeling with ECG data is that there are multiple outcomes (different ECG features). For example, in the card package, the geh dataset contains several ECG features based on vectorcardiography. library(card) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.4 ✓ dplyr 1.0.2 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() data(geh) names(geh) ## [1] &quot;pid&quot; &quot;hhp_id&quot; &quot;age&quot; ## [4] &quot;sex&quot; &quot;age_cat&quot; &quot;systolic_bp_first&quot; ## [7] &quot;systolic_bp_second&quot; &quot;systolic_bp_third&quot; &quot;diastolic_bp_first&quot; ## [10] &quot;diastolic_bp_second&quot; &quot;diastolic_bp_third&quot; &quot;pulse_rate_first&quot; ## [13] &quot;pulse_rate_second&quot; &quot;height_cm&quot; &quot;weight_kg&quot; ## [16] &quot;waist_cm&quot; &quot;dia_trt_allopdrug&quot; &quot;hbp_trt_allopdrug&quot; ## [19] &quot;hyp_trt_allopdrug&quot; &quot;lab_hba1c&quot; &quot;lab_fasting_bg&quot; ## [22] &quot;lab_fasting_insulin&quot; &quot;lab_tchol&quot; &quot;lab_ldlchol&quot; ## [25] &quot;lab_hdlchol&quot; &quot;lab_triglyc&quot; &quot;lab_ser_urea&quot; ## [28] &quot;lab_ser_creatinine&quot; &quot;lab_urin_malbumin&quot; &quot;pd_heart&quot; ## [31] &quot;bmi&quot; &quot;bmi_cat&quot; &quot;obese&quot; ## [34] &quot;obese_asian&quot; &quot;sbp_mean&quot; &quot;dbp_mean&quot; ## [37] &quot;pulse_mean&quot; &quot;htn&quot; &quot;cad&quot; ## [40] &quot;drugs_dm&quot; &quot;dm&quot; &quot;dm_lab&quot; ## [43] &quot;dm_control&quot; &quot;dm_pre&quot; &quot;homa&quot; ## [46] &quot;high_waist&quot; &quot;high_tchol&quot; &quot;high_ldl&quot; ## [49] &quot;low_hdl&quot; &quot;high_triglyc&quot; &quot;met_syn_num&quot; ## [52] &quot;met_syn&quot; &quot;pr_interval&quot; &quot;p_duration&quot; ## [55] &quot;p_amp&quot; &quot;qrs_duration&quot; &quot;qt_interval&quot; ## [58] &quot;cornell_voltage&quot; &quot;nhanes_score&quot; &quot;svg_mag&quot; ## [61] &quot;az_svg&quot; &quot;az_svg_m&quot; &quot;el_svg&quot; ## [64] &quot;el_svg_m&quot; &quot;qrs_tang&quot; &quot;auc_vm_qt&quot; ## [67] &quot;wvg&quot; &quot;log_svg&quot; &quot;log_auc_qt&quot; ## [70] &quot;log_wvg&quot; The first issue is the causal model, which can be visualized using a directed acyclic graph. The variables of interest are a subset of the dataset. In this case, we’re looking at the relationship of diabetes with cardiotoxicity in a very small subset of participants. library(ggdag) ## ## Attaching package: &#39;ggdag&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## filter bd &lt;- dagify( GEH ~ DM + Age + BMI + HTN + CAD + IR + Sex, DM ~ Age + IR + BMI, CAD ~ DM + BMI + HTN + Age + Sex, HTN ~ Age, IR ~ BMI + Age, Age ~ Sex, exposure = &quot;DM&quot;, outcome = &quot;GEH&quot; ) d1 &lt;- ggdag_parents(bd, &quot;DM&quot;, layout = &quot;star&quot;) + theme_dag() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Factors Affecting Diabetes&quot;) d2 &lt;- ggdag_parents(bd, &quot;GEH&quot;, layout = &quot;star&quot;) + theme_dag() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Factors Affecting GEH&quot;) # Combine and plot gridExtra::grid.arrange(d1, d2, nrow = 1) As we can see, many things effect ECG findings, and a subgroup of those impact diabetes, suggesting a number of potential effect modifiers and potential confounders/mediators. Using a sequential model building method in the card package allows for a simple way to perform this analysis. This will build a linear model for each outcome, and repeat the model with an additional covariate in the sequence of listed in the formula. # Select variables vars &lt;- c(&quot;svg_mag&quot;, &quot;az_svg&quot;, &quot;el_svg&quot;, &quot;qrs_tang&quot;, &quot;log_auc_qt&quot;, &quot;log_wvg&quot;, &quot;lab_hba1c&quot;, &quot;lab_fasting_bg&quot;, &quot;homa&quot;, &quot;dm&quot;, &quot;age&quot;, &quot;bmi&quot;, &quot;bmi_cat&quot;, &quot;age_cat&quot;, &quot;sex&quot;, &quot;htn&quot;, &quot;cad&quot;, &quot;lab_ser_creatinine&quot;, &quot;lab_tchol&quot;) test_data &lt;- geh %&gt;% select(all_of(vars)) %&gt;% #na.omit() %&gt;% #filter(homa &lt;= 5 * sd(homa, na.rm = TRUE)) %&gt;% # Remove outliers mutate( bmi_cat = factor(bmi_cat, levels = c(0:3), labels = c(&quot;Underweight&quot;, &quot;Normal&quot;, &quot;Overweight&quot;, &quot;Obese&quot;)), age_cat = factor(age_cat, levels = c(0:2), labels = c(&quot;&lt;45&quot;, &quot;45-65&quot;, &quot;&gt;65&quot;)), sex = factor(sex, levels = c(0,1), labels = c(&quot;Female&quot;, &quot;Male&quot;)) ) %&gt;% mutate(across( c(svg_mag, az_svg, el_svg, qrs_tang, log_auc_qt, log_wvg), function(x) { as.vector(scale(x, center = TRUE, scale = TRUE)) } )) # Sequential model building models &lt;- card::build_models( svg_mag + az_svg + el_svg + qrs_tang + log_auc_qt + log_wvg ~ lab_hba1c + age + sex + bmi + cad + htn, data = test_data, exposure = &quot;lab_hba1c&quot;, engine = &quot;linear&quot;, type = &quot;sequential&quot; ) head(models) ## # A tibble: 6 x 9 ## outcomes term estimate std.error statistic p.value conf.low conf.high covar ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 svg_mag (Inter… 0.339 0.196 1.73 0.0847 -0.0466 0.724 1 ## 2 svg_mag lab_hb… -0.0412 0.0234 -1.76 0.0796 -0.0872 0.00488 1 ## 3 az_svg (Inter… -0.315 0.195 -1.62 0.107 -0.698 0.0681 1 ## 4 az_svg lab_hb… 0.0381 0.0233 1.64 0.102 -0.00763 0.0839 1 ## 5 el_svg (Inter… 0.0767 0.195 0.394 0.694 -0.306 0.460 1 ## 6 el_svg lab_hb… -0.00862 0.0233 -0.370 0.711 -0.0544 0.0371 1 To assess or get a sense of how the variables are playing out, we can visualize the estimates across the builds of the models. This will also use the gganimate package to show the effect of data layering # Libraries library(gganimate) library(ggthemes) # Data df &lt;- models %&gt;% # Remove intercepts filter(term != &quot;(Intercept)&quot;) %&gt;% # Sequence the terms mutate( term = factor( term, levels = c(&quot;lab_hba1c&quot;, &quot;age&quot;, &quot;sexMale&quot;, &quot;bmi&quot;, &quot;cad1&quot;, &quot;htn1&quot;), labels = c(&quot;HbA1c&quot;, &quot;Age&quot;, &quot;Sex&quot;, &quot;BMI&quot;, &quot;CAD&quot;, &quot;HTN&quot;) ) ) # ggplot g &lt;- ggplot(df, aes(x = factor(covar), y = estimate, color = term)) + facet_wrap(~outcomes, scales = &quot;fixed&quot;) + geom_point( aes(color = term), data = filter(df, p.value &gt;= 0.20), shape = 1, position = &quot;jitter&quot; ) + geom_point( aes(color = term), data = filter(df, p.value &lt; 0.20), shape = 19, position = &quot;jitter&quot; ) + scale_color_ptol(name = &quot;Predictors&quot;) + theme_minimal() + theme( legend.position = &quot;bottom&quot;, legend.box = &quot;horizontal&quot;, panel.border = element_rect(colour = &quot;black&quot;, fill = NA) ) + labs( title = &quot;Estimates in Sequential Models&quot;, x = &quot;Number of Covariates in Model&quot;, y = &quot;GEH Parameters (z-normalized)&quot; ) # Animated a &lt;- g + transition_reveal(covar) animate(a, end_pause = 30) "],["building-a-modeling-matrix-function.html", "8.2 Building a Modeling Matrix Function", " 8.2 Building a Modeling Matrix Function One issue that has occurred is that using the function build_models() is the idea that a prespecified formula can be used to generate a large number of models, so we can assess the impact of each variable on the model. However, in R, the limitation is that each regression package is unique in how it is specified. One option is to rely on the prespecified parsnip models that unify regression modeling formulas. 8.2.1 A tidy Approach to Multiple Outcomes and Predictors Here is a base example that I hope to build off of (generated by Julia Silge for an issue filed on the workflows package). library(tidymodels) ## ── Attaching packages ────────────────────────────────────── tidymodels 0.1.2 ── ## ✓ broom 0.7.2.9000 ✓ recipes 0.1.15 ## ✓ dials 0.0.9 ✓ rsample 0.0.8 ## ✓ infer 0.5.3 ✓ tune 0.1.2 ## ✓ modeldata 0.1.0 ✓ workflows 0.2.1 ## ✓ parsnip 0.1.4 ✓ yardstick 0.0.7 ## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── ## x scales::discard() masks purrr::discard() ## x ggdag::filter() masks dplyr::filter(), stats::filter() ## x recipes::fixed() masks stringr::fixed() ## x dplyr::lag() masks stats::lag() ## x yardstick::spec() masks readr::spec() ## x recipes::step() masks stats::step() library(vctrs) ## ## Attaching package: &#39;vctrs&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## data_frame ## The following object is masked from &#39;package:tibble&#39;: ## ## data_frame #&gt; Attaching package: &#39;vctrs&#39; #&gt; The following object is masked from &#39;package:tibble&#39;: #&gt; #&gt; data_frame #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; data_frame outcome &lt;- &quot;mpg&quot; predictors &lt;- setdiff(names(mtcars), outcome) # Specify parsnip model to be used lm_spec &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) ## make a little function to create a workflow with `mpg` as outcome and our set of predictors wf_seq &lt;- function(preds) { workflow() %&gt;% add_model(lm_spec) %&gt;% add_variables(outcomes = mpg, predictors = !!preds) } ## set up the &quot;sequential&quot; set of predictors and create each workflow, then fit tibble(num_preds = 1:length(predictors)) %&gt;% mutate(preds = map(num_preds, ~vec_slice(predictors, 1:.))) %&gt;% mutate(wf = map(preds, wf_seq), fitted_wf = map(wf, fit, mtcars)) ## # A tibble: 10 x 4 ## num_preds preds wf fitted_wf ## &lt;int&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 1 &lt;chr [1]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 2 2 &lt;chr [2]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 3 3 &lt;chr [3]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 4 4 &lt;chr [4]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 5 5 &lt;chr [5]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 6 6 &lt;chr [6]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 7 7 &lt;chr [7]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 8 8 &lt;chr [8]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 9 9 &lt;chr [9]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 10 10 &lt;chr [10]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; # A tibble: 10 x 4 #&gt; num_preds preds wf fitted_wf #&gt; &lt;int&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;chr [1]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 2 2 &lt;chr [2]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 3 3 &lt;chr [3]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 4 4 &lt;chr [4]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 5 5 &lt;chr [5]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 6 6 &lt;chr [6]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 7 7 &lt;chr [7]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 8 8 &lt;chr [8]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 9 9 &lt;chr [9]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 10 10 &lt;chr [10]&gt; &lt;workflow&gt; &lt;workflow&gt; This goes back to the approach listed in R4DS that shows the purrr method of regressions with the map() function. A data frame that described this would be likely the most succinct way to handle this issue, using several specified elements: a single dataframe that was made, to be used throughout column to describe the number of predictors or to help identify each row column that contains the predictors column that contains the outcomes (allowing for combinations) column that specifies the exposures (or fixed effects) column that contains model specifications using parsnip column that contains workflows To set this up, likely there will need to be several functions/steps: Creating a modeling “matrix” as above Updating or allowing modifications to the table to occur, such as combining additional matrices, etc Fitting the models using the specified workflows The primary structure here will be a tibble that contains the basic parameters that can be extracted at the end. 8.2.1.1 Creating a modeling matrix Here we can see a slighly modified set of sequences to create a data frame that would hold the workflow needed for a regression analysis, but without the actual results or fit. # Would like to use a formula approach to create this matrix f &lt;- svg_mag + az_svg + el_svg + qrs_tang + log_auc_qt + log_wvg ~ lab_hba1c + age + sex + bmi + cad + htn # Left and right side, and length of each outcomes &lt;- all.vars(f[[2]]) predictors &lt;- all.vars(f[[3]]) n_outcomes &lt;- length(outcomes) n_predictors &lt;- length(predictors) # Assuming this will be a linear regression lm_mod &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) wf_seq &lt;- function(outs, preds, mods) { workflow() %&gt;% add_model(mods) %&gt;% add_variables(outcomes = !!outs, predictors = !!preds) } all_models &lt;- tibble(n_covar = 1:length(predictors)) %&gt;% mutate(pred = map(n_covar, ~vec_slice(predictors, 1:.))) %&gt;% expand_grid(., out = outcomes) %&gt;% mutate(spec = list(lm_mod)) %&gt;% mutate(wf = pmap(list(out, pred, spec), wf_seq)) Thus, the modeling matrix is a plan that will be used and should likely exist as a moldable object. That object can then be shaped into a table of columns going over the types of models to be run. Finally, the table can be analyzed. This fits the epidemiological or research workflow in that general aims for the research are decided, hypotheses are generated, and finally statistical analysis is performed. plan() should take in a formula, specific characteristics, and create a template update() should allow the plan to be modified or updated model() will allow for fitting the models defined This is just a first draft at a thoughtful approach, and further iterations should allow for a more fluid, conversational function set. # Data set to be used was generated above df &lt;- test_data # The above made modeling table can then be fit final &lt;- all_models %&gt;% mutate(fit = map(wf, fit, data = df)) "],["building-a-parsnip-model.html", "8.3 Building a parsnip Model", " 8.3 Building a parsnip Model This is an example of going through and creating a modeling interface using circular regressions, which are found in the {circular} package. Here is an example of using the packages to do the traditional modeling approach. # Set up of formulas, and the matrices library(card) data(geh) f &lt;- az_svg ~ lab_hba1c + age + sex + bmi + cad + htn mat &lt;- model.frame(f, geh) mat$az_svg &lt;- circular::circular(mat$az_svg, units = &quot;degrees&quot;) x &lt;- model.matrix(f, mat) y &lt;- mat$az_svg # Using hte circular package library(circular) ## ## Attaching package: &#39;circular&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## sd, var lm.circular(y = y, x = x, type = &quot;c-l&quot;, init = rep(0, ncol(x)), tol = 1e-3, verbose = TRUE) ## Iteration 1 : Log-Likelihood = 545.2841 ## Iteration 2 : Log-Likelihood = 545.2849 ## Iteration 3 : Log-Likelihood = 545.2849 ## ## Call: ## lm.circular.cl(y = ..1, x = ..2, init = ..3, verbose = TRUE, tol = 0.001) ## ## ## Circular-Linear Regression ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## [1,] -0.0280548 0.1068864 0.262 0.396 ## [2,] 0.0055406 0.0057706 0.960 0.168 ## [3,] -0.0001874 0.0010295 0.182 0.428 ## [4,] -0.0235423 0.0235008 1.002 0.158 ## [5,] -0.0001335 0.0021729 0.061 0.475 ## [6,] -0.0186406 0.0349852 0.533 0.297 ## [7,] 0.0129938 0.0295104 0.440 0.330 ## ## Log-Likelihood: 545.3 ## ## Summary: (mu in radians) ## mu: 7.685 ( 1.283 ) kappa: 4.742 ( 0.2822 ) ## p-values are approximated using normal distribution 8.3.1 Setting up the model specifications This process sets up the parsnip model. # Start making new model parsnip::set_new_model(&quot;circular_reg&quot;) # Add parsnip models to another package parsnip::set_model_mode(model = &quot;circular_reg&quot;, mode = &quot;regression&quot;) parsnip::set_model_engine(&quot;circular_reg&quot;, mode = &quot;regression&quot;, eng = &quot;circular&quot;) parsnip::set_dependency(&quot;circular_reg&quot;, eng = &quot;circular&quot;, pkg = &quot;circular&quot;) # Arguments = type parsnip::set_model_arg( model = &quot;circular_reg&quot;, eng = &quot;circular&quot;, parsnip = &quot;pattern&quot;, original = &quot;type&quot;, func = list(pkg = &quot;circular&quot;, fun = &quot;lm.circular&quot;), has_submodel = FALSE ) # Arguments = init parsnip::set_model_arg( model = &quot;circular_reg&quot;, eng = &quot;circular&quot;, parsnip = &quot;initial&quot;, original = &quot;init&quot;, func = list(pkg = &quot;circular&quot;, fun = &quot;lm.circular&quot;), has_submodel = FALSE ) # Arguments = tol parsnip::set_model_arg( model = &quot;circular_reg&quot;, eng = &quot;circular&quot;, parsnip = &quot;tolerance&quot;, original = &quot;tol&quot;, func = list(pkg = &quot;circular&quot;, fun = &quot;lm.circular&quot;), has_submodel = FALSE ) # Encoding parsnip::set_encoding( model = &quot;circular_reg&quot;, eng = &quot;circular&quot;, mode = &quot;regression&quot;, options = list( predictor_indicators = &quot;traditional&quot;, compute_intercept = TRUE, remove_intercept = FALSE, allow_sparse_x = TRUE ) ) # Fit parsnip::set_fit( model = &quot;circular_reg&quot;, eng = &quot;circular&quot;, mode = &quot;regression&quot;, value = list( interface = &quot;matrix&quot;, protect = c(&quot;x&quot;, &quot;y&quot;), func = c(pkg = &quot;circular&quot;, fun = &quot;lm.circular&quot;), defaults = list(verbose = TRUE) ) ) # Prediction parsnip::set_pred( model = &quot;circular_reg&quot;, eng = &quot;circular&quot;, mode = &quot;regression&quot;, type = &quot;numeric&quot;, value = list( pre = NULL, post = NULL, func = c(fun = &quot;predict&quot;), args = list( object = quote(object$fit), new_data = quote(new_data), type = &quot;numeric&quot; ) ) ) # Official parsnip model spec circular_reg &lt;- function(mode = &quot;regression&quot;, pattern = NULL, initial = NULL, tolerance = NULL) { # Check correct mode if(mode != &quot;regression&quot;) { stop(&quot;`mode` should be &#39;regression&#39;&quot;, call. = FALSE) } # Capture arguments args &lt;- list( pattern = rlang::enquo(pattern), initial = rlang::enquo(initial), tolerance = rlang::enquo(tolerance) ) # Model specs / slots parsnip::new_model_spec( &quot;circular_reg&quot;, args = args, mode = mode, eng_args = NULL, method = NULL, engine = NULL ) } Now we can check to see if the model specifications make sense are are literate. "]]
=======
[["models.html", "Chapter 8 Modeling", " Chapter 8 Modeling Here is a collection of explorations on modeling approaches as they related to electrocardiography and epidemiology. "],["modeling-multiple-outcomes-and-predictors.html", "8.1 Modeling Multiple Outcomes and Predictors", " 8.1 Modeling Multiple Outcomes and Predictors A recurrent issue with causality-focused modeling with ECG data is that there are multiple outcomes (different ECG features). For example, in the card package, the geh dataset contains several ECG features based on vectorcardiography. 8.1.1 Creating Multiple Models library(card) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.4 ✓ dplyr 1.0.2 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() data(geh) names(geh) ## [1] &quot;pid&quot; &quot;hhp_id&quot; &quot;age&quot; ## [4] &quot;sex&quot; &quot;age_cat&quot; &quot;systolic_bp_first&quot; ## [7] &quot;systolic_bp_second&quot; &quot;systolic_bp_third&quot; &quot;diastolic_bp_first&quot; ## [10] &quot;diastolic_bp_second&quot; &quot;diastolic_bp_third&quot; &quot;pulse_rate_first&quot; ## [13] &quot;pulse_rate_second&quot; &quot;height_cm&quot; &quot;weight_kg&quot; ## [16] &quot;waist_cm&quot; &quot;dia_trt_allopdrug&quot; &quot;hbp_trt_allopdrug&quot; ## [19] &quot;hyp_trt_allopdrug&quot; &quot;lab_hba1c&quot; &quot;lab_fasting_bg&quot; ## [22] &quot;lab_fasting_insulin&quot; &quot;lab_tchol&quot; &quot;lab_ldlchol&quot; ## [25] &quot;lab_hdlchol&quot; &quot;lab_triglyc&quot; &quot;lab_ser_urea&quot; ## [28] &quot;lab_ser_creatinine&quot; &quot;lab_urin_malbumin&quot; &quot;pd_heart&quot; ## [31] &quot;bmi&quot; &quot;bmi_cat&quot; &quot;obese&quot; ## [34] &quot;obese_asian&quot; &quot;sbp_mean&quot; &quot;dbp_mean&quot; ## [37] &quot;pulse_mean&quot; &quot;htn&quot; &quot;cad&quot; ## [40] &quot;drugs_dm&quot; &quot;dm&quot; &quot;dm_lab&quot; ## [43] &quot;dm_control&quot; &quot;dm_pre&quot; &quot;homa&quot; ## [46] &quot;high_waist&quot; &quot;high_tchol&quot; &quot;high_ldl&quot; ## [49] &quot;low_hdl&quot; &quot;high_triglyc&quot; &quot;met_syn_num&quot; ## [52] &quot;met_syn&quot; &quot;pr_interval&quot; &quot;p_duration&quot; ## [55] &quot;p_amp&quot; &quot;qrs_duration&quot; &quot;qt_interval&quot; ## [58] &quot;cornell_voltage&quot; &quot;nhanes_score&quot; &quot;svg_mag&quot; ## [61] &quot;az_svg&quot; &quot;az_svg_m&quot; &quot;el_svg&quot; ## [64] &quot;el_svg_m&quot; &quot;qrs_tang&quot; &quot;auc_vm_qt&quot; ## [67] &quot;wvg&quot; &quot;log_svg&quot; &quot;log_auc_qt&quot; ## [70] &quot;log_wvg&quot; The first issue is the causal model, which can be visualized using a directed acyclic graph. The variables of interest are a subset of the dataset. In this case, we’re looking at the relationship of diabetes with cardiotoxicity in a very small subset of participants. library(ggdag) ## ## Attaching package: &#39;ggdag&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## filter bd &lt;- dagify( GEH ~ DM + Age + BMI + HTN + CAD + IR + Sex, DM ~ Age + IR + BMI, CAD ~ DM + BMI + HTN + Age + Sex, HTN ~ Age, IR ~ BMI + Age, Age ~ Sex, exposure = &quot;DM&quot;, outcome = &quot;GEH&quot; ) d1 &lt;- ggdag_parents(bd, &quot;DM&quot;, layout = &quot;star&quot;) + theme_dag() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Factors Affecting Diabetes&quot;) d2 &lt;- ggdag_parents(bd, &quot;GEH&quot;, layout = &quot;star&quot;) + theme_dag() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Factors Affecting GEH&quot;) # Combine and plot gridExtra::grid.arrange(d1, d2, nrow = 1) As we can see, many things effect ECG findings, and a subgroup of those impact diabetes, suggesting a number of potential effect modifiers and potential confounders/mediators. Using a sequential model building method in the card package allows for a simple way to perform this analysis. This will build a linear model for each outcome, and repeat the model with an additional covariate in the sequence of listed in the formula. # Select variables vars &lt;- c(&quot;svg_mag&quot;, &quot;az_svg&quot;, &quot;el_svg&quot;, &quot;qrs_tang&quot;, &quot;log_auc_qt&quot;, &quot;log_wvg&quot;, &quot;lab_hba1c&quot;, &quot;lab_fasting_bg&quot;, &quot;homa&quot;, &quot;dm&quot;, &quot;age&quot;, &quot;bmi&quot;, &quot;bmi_cat&quot;, &quot;age_cat&quot;, &quot;sex&quot;, &quot;htn&quot;, &quot;cad&quot;, &quot;lab_ser_creatinine&quot;, &quot;lab_tchol&quot;) df &lt;- geh %&gt;% select(all_of(vars)) %&gt;% #na.omit() %&gt;% #filter(homa &lt;= 5 * sd(homa, na.rm = TRUE)) %&gt;% # Remove outliers mutate( bmi_cat = factor(bmi_cat, levels = c(0:3), labels = c(&quot;Underweight&quot;, &quot;Normal&quot;, &quot;Overweight&quot;, &quot;Obese&quot;)), age_cat = factor(age_cat, levels = c(0:2), labels = c(&quot;&lt;45&quot;, &quot;45-65&quot;, &quot;&gt;65&quot;)), sex = factor(sex, levels = c(0,1), labels = c(&quot;Female&quot;, &quot;Male&quot;)) ) %&gt;% mutate(across( c(svg_mag, az_svg, el_svg, qrs_tang, log_auc_qt, log_wvg), function(x) { as.vector(scale(x, center = TRUE, scale = TRUE)) } )) # Sequential model building models &lt;- card::build_models( svg_mag + az_svg + el_svg + qrs_tang + log_auc_qt + log_wvg ~ lab_hba1c + age + sex + bmi + cad + htn, data = df, exposure = &quot;lab_hba1c&quot;, engine = &quot;linear&quot;, type = &quot;sequential&quot; ) head(models) ## # A tibble: 6 x 9 ## outcomes term estimate std.error statistic p.value conf.low conf.high covar ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 svg_mag (Inter… 0.339 0.196 1.73 0.0847 -0.0466 0.724 1 ## 2 svg_mag lab_hb… -0.0412 0.0234 -1.76 0.0796 -0.0872 0.00488 1 ## 3 az_svg (Inter… -0.315 0.195 -1.62 0.107 -0.698 0.0681 1 ## 4 az_svg lab_hb… 0.0381 0.0233 1.64 0.102 -0.00763 0.0839 1 ## 5 el_svg (Inter… 0.0767 0.195 0.394 0.694 -0.306 0.460 1 ## 6 el_svg lab_hb… -0.00862 0.0233 -0.370 0.711 -0.0544 0.0371 1 8.1.2 Visualize Regression Estimates To assess or get a sense of how the variables are playing out, we can visualize the estimates across the builds of the models. This will also use the gganimate package to show the effect of data layering # Libraries library(gganimate) library(ggthemes) # Data df &lt;- models %&gt;% # Remove intercepts filter(term != &quot;(Intercept)&quot;) %&gt;% # Sequence the terms mutate( term = factor( term, levels = c(&quot;lab_hba1c&quot;, &quot;age&quot;, &quot;sexMale&quot;, &quot;bmi&quot;, &quot;cad1&quot;, &quot;htn1&quot;), labels = c(&quot;HbA1c&quot;, &quot;Age&quot;, &quot;Sex&quot;, &quot;BMI&quot;, &quot;CAD&quot;, &quot;HTN&quot;) ) ) # ggplot g &lt;- ggplot(df, aes(x = factor(covar), y = estimate, color = term)) + facet_wrap(~outcomes, scales = &quot;fixed&quot;) + geom_point( aes(color = term), data = filter(df, p.value &gt;= 0.20), shape = 1, position = &quot;jitter&quot; ) + geom_point( aes(color = term), data = filter(df, p.value &lt; 0.20), shape = 19, position = &quot;jitter&quot; ) + scale_color_ptol(name = &quot;Predictors&quot;) + theme_minimal() + theme( legend.position = &quot;bottom&quot;, legend.box = &quot;horizontal&quot;, panel.border = element_rect(colour = &quot;black&quot;, fill = NA) ) + labs( title = &quot;Estimates in Sequential Models&quot;, x = &quot;Number of Covariates in Model&quot;, y = &quot;GEH Parameters (z-normalized)&quot; ) # Animated a &lt;- g + transition_reveal(covar) animate(a, end_pause = 30) "],["building-a-modeling-matrix.html", "8.2 Building a Modeling Matrix", " 8.2 Building a Modeling Matrix One issue that has occurred is that using the function build_models() is the idea that a prespecified formula can be used to generate a large number of models, so we can assess the impact of each variable on the model. However, in R, the limitation is that each regression package is unique in how it is specified. One option is to rely on the prespecified parsnip models that unify regression modeling formulas. 8.2.1 A tidy Approach to Multiple Outcomes and Predictors Here is a base example that I hope to build off of (generated by Julia Silge for an issue filed on the workflows package). library(tidymodels) ## ── Attaching packages ────────────────────────────────────── tidymodels 0.1.1 ── ## ✓ broom 0.7.2.9000 ✓ recipes 0.1.14 ## ✓ dials 0.0.9 ✓ rsample 0.0.8 ## ✓ infer 0.5.3 ✓ tune 0.1.1 ## ✓ modeldata 0.1.0 ✓ workflows 0.2.1 ## ✓ parsnip 0.1.3 ✓ yardstick 0.0.7 ## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── ## x scales::discard() masks purrr::discard() ## x ggdag::filter() masks dplyr::filter(), stats::filter() ## x recipes::fixed() masks stringr::fixed() ## x dplyr::lag() masks stats::lag() ## x yardstick::spec() masks readr::spec() ## x recipes::step() masks stats::step() library(vctrs) ## ## Attaching package: &#39;vctrs&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## data_frame ## The following object is masked from &#39;package:tibble&#39;: ## ## data_frame #&gt; Attaching package: &#39;vctrs&#39; #&gt; The following object is masked from &#39;package:tibble&#39;: #&gt; #&gt; data_frame #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; data_frame outcome &lt;- &quot;mpg&quot; predictors &lt;- setdiff(names(mtcars), outcome) # Specify parsnip model to be used lm_spec &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) ## make a little function to create a workflow with `mpg` as outcome and our set of predictors wf_seq &lt;- function(preds) { workflow() %&gt;% add_model(lm_spec) %&gt;% add_variables(outcomes = mpg, predictors = !!preds) } ## set up the &quot;sequential&quot; set of predictors and create each workflow, then fit tibble(num_preds = 1:length(predictors)) %&gt;% mutate(preds = map(num_preds, ~vec_slice(predictors, 1:.))) %&gt;% mutate(wf = map(preds, wf_seq), fitted_wf = map(wf, fit, mtcars)) ## # A tibble: 10 x 4 ## num_preds preds wf fitted_wf ## &lt;int&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 1 &lt;chr [1]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 2 2 &lt;chr [2]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 3 3 &lt;chr [3]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 4 4 &lt;chr [4]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 5 5 &lt;chr [5]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 6 6 &lt;chr [6]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 7 7 &lt;chr [7]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 8 8 &lt;chr [8]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 9 9 &lt;chr [9]&gt; &lt;workflow&gt; &lt;workflow&gt; ## 10 10 &lt;chr [10]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; # A tibble: 10 x 4 #&gt; num_preds preds wf fitted_wf #&gt; &lt;int&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;chr [1]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 2 2 &lt;chr [2]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 3 3 &lt;chr [3]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 4 4 &lt;chr [4]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 5 5 &lt;chr [5]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 6 6 &lt;chr [6]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 7 7 &lt;chr [7]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 8 8 &lt;chr [8]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 9 9 &lt;chr [9]&gt; &lt;workflow&gt; &lt;workflow&gt; #&gt; 10 10 &lt;chr [10]&gt; &lt;workflow&gt; &lt;workflow&gt; This goes back to the approach listed in R4DS that shows the purrr method of regressions with the map() function. A data frame that described "]]
>>>>>>> 82bd0d36d1a9f5f20f8528d135d479da454ac798
