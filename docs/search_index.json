[
["index.html", "Thoughts Preface", " Thoughts Anish Shah 2020-07-04 Preface This initially was designed to help document exploratory concepts that were developed using the card package, but also expanded to clinical concepts leveraging the power of R and Rmarkdown. As this develops, it will likely create its own order. Currently it revolves around the principal components: circadian rhythm electrocardiography autonomic physiology clinical medicine programming "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction An introduction would go here. "],
["r-and-rstudio.html", "1.1 R and RStudio", " 1.1 R and RStudio I have been learning R since 2010, and have found it to be an excellent: open-source excellent community an outstanding IDE with RStudio package development Currently using R version 4.0.0 (2020-04-24), nicknamed Arbor Day, with RStudio 1.3. "],
["git-and-github.html", "1.2 git and Github", " 1.2 git and Github "],
["setup.html", "Chapter 2 Technical Setup", " Chapter 2 Technical Setup A workflow outside of research and medicine should be a natural corollary to it, taking similar approaches to problems to help making things simpler (instead of more complicated). Certain programs, approaches, and philosophies have made this possible, detailed below. vim - text editor mutt - email client git "],
["vim.html", "2.1 Vim", " 2.1 Vim Vim is much more than a text editor. "],
["mutt.html", "2.2 Mutt", " 2.2 Mutt Email clients are typically bloated with additional “features” that are supposed to improve the email experience. However, it seems that these additions in a GUI interface actually slow things down. Mutt, and its heavily-featured fork Neomutt, are ways around those problems with a command line interface. Email is not just a single service, but a bundling of several components. mutt is a MUA (mail user agent) that is a front-end for users to manage stored messages in a mailbox msmtp is a MTA (mail transport agent) that sends mails through SMTP (simple mail transfer protocol) mbsync is a MRA (mail retrieval agent) that can hop onto a mail server and actually retrieve items from inboxes Originally, for my personal gmail and professional/institution, the following resources were helpful. https://wincent.com/blog/email http://stevelosh.com/blog/2012/10/the-homely-mutt/ https://gitlab.com/muttmua/mutt/wikis/MuttGuide https://www.youtube.com/watch?v=2jMInHnpNfQ&amp;t=111s https://webgefrickel.de/blog/a-modern-mutt-setup https://webgefrickel.de/blog/a-modern-mutt-setup-part-two "],
["building-a-cv.html", "2.3 Building a CV", " 2.3 Building a CV Instead of storing the elements of a CV in a WYSIWYG format, an alternative approach is to use separate files that can be compiled together to produce a CV in a data-driven pipeline. This process was inspired by the datadrivencv and vitae packages. Reference links: https://slides.mitchelloharawild.com/vitae/#1 https://github.com/ropensci/rorcid https://github.com/robjhyndman/CV "],
["circadian.html", "Chapter 3 Circadian Physiology ", " Chapter 3 Circadian Physiology "],
["chronobiology.html", "3.1 Chronobiology", " 3.1 Chronobiology Will discuss circadian biology/physiology. "],
["circadian-disruption.html", "3.2 Circadian Disruption", " 3.2 Circadian Disruption "],
["references.html", "3.3 References", " 3.3 References "],
["cosinor.html", "Chapter 4 Cosinor Analysis", " Chapter 4 Cosinor Analysis The issue with time series analysis is that the data is by its nature circular and thus cannot be easily be analyzed through traditional, linear methods. The following is the development/expansion of the cosinor model to help study circadian rhythms (3) using R. The card package was developed to help tackle this problem. # Library library(card) library(tidyverse) ## ── Attaching packages ──────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.1 ✓ dplyr 1.0.0 ## ✓ tidyr 1.1.0 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ─────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() # Dataset data(&quot;twins&quot;) # Example of data ggplot(twins, aes(x = hour, y = rDYX)) + geom_smooth(method = &quot;gam&quot;, se = TRUE) ## `geom_smooth()` using formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; "],
["overview.html", "4.1 Overview", " 4.1 Overview Using the cosinor() function, the characteristics of the circadian pattern can be retrieved. # Cosinor model m &lt;- cosinor(rDYX ~ hour, twins, tau = 24) summary(m) ## Individual Cosinor Model ## ------------------------------------------ ## Call: ## cosinor(formula = rDYX ~ M + A1 * cos(2*pi*hour/24 + phi1) ## ## Period(s): 24 ## ## Residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -3.12633 -0.53228 -0.03597 0.00000 0.49132 4.82150 ## ## Coefficients: ## Estimate Std. Error ## mesor 2.8604855 0.006098624 ## amp1 0.2986101 0.008746706 ## phi1 -2.6687044 0.028860014 The statistical principles behind this method allow for different methods to model, diagnose, and interpret findings. (Refinetti, Cornélissen, and Halberg 2007; Cornelissen 2014) single component cosinor multiple component cosinor population cosinor confidence intervals (ellipse method) zero amplitude test lack-of-fit testing The example here use the dataset twins which contains a continuous ECG signal, called DYX, collected at hourly time points. References "],
["single-component-cosinor.html", "4.2 Single component cosinor", " 4.2 Single component cosinor The single component cosinor method is modeled as: \\[Y(t) = M + A cos(\\frac{2 \\pi t}{\\tau} + \\phi) + \\epsilon\\] Where: \\[ \\begin{aligned} M &amp;= MESOR\\ (midline\\ estimating\\ statistic\\ of\\ rhythm) \\\\ t &amp;= time in hours \\\\ \\epsilon &amp;= error \\\\ \\phi &amp;= acrophase \\\\ \\tau &amp;= tau\\ (period) \\\\ \\end{aligned} \\] To model this function, it must be transformed linearly to assess the coefficients. \\[Y(t) = M + \\beta x_{t} + \\gamma z_{t} + \\epsilon_{t}\\] The new coefficients and parameters are defined as: \\[ \\begin{aligned} \\beta &amp;= A cos(\\phi) \\\\ \\gamma &amp;= -A sin(\\phi) \\\\ x_{t} &amp;= cos(\\frac{2 \\pi t}{\\tau}) \\\\ z_{t} &amp;= sin(\\frac{2 \\pi t}{\\tau}) \\\\ \\end{aligned} \\] In the twins data, the time value \\(t\\) is measured in hours. As this is 24-hour data, the assumption is that \\(\\tau = 24\\). df &lt;- subset(twins, patid == 60) # Single individual cosinor y &lt;- df$rDYX t &lt;- df$hour n &lt;- length(t) # Number of observations period &lt;- 24 # Transformed variables x &lt;- cos(( 2 * pi * t) / period) z &lt;- sin(( 2 * pi * t) / period) To generate the coefficients in R requires sovling a matrix of normal/linear equations. # Matrices ymat &lt;- as.matrix(cbind(y = c(sum(y), sum(y * x), sum(y * z)))) mcol &lt;- c(n, sum(x), sum(z)) # Mesor column bcol &lt;- c(sum(x), sum(x^2), sum(x * z)) # Beta column gcol &lt;- c(sum(z), sum(x * z), sum(z^2)) # Gamma column xmat &lt;- as.matrix(cbind(m = mcol, b = bcol, g = gcol)) # Solution coefs &lt;- solve(t(xmat) %*% xmat, tol = 1e-21) %*% (t(xmat) %*% ymat) mesor &lt;- coefs[1] # mesor beta &lt;- coefs[2] # beta gamma &lt;- coefs[3] # gamma For a single cosinor, as in, the analysis of the values from a single individual over 1 period, the values for the Amplitude (\\(A\\)) and Acrophase (\\(\\phi\\)) can be calculated. \\[ \\begin{aligned} A &amp;= \\sqrt{(\\beta^2 + \\gamma^2)} \\phi &amp;= k \\pi + g \\times arctan(\\frac{\\gamma}{\\beta}) \\end{aligned} \\] Because the values of \\(\\gamma\\) and \\(\\beta\\) represent trigonemtric values, the position or quadrant of the circle changes the value of \\(\\phi\\). \\(\\beta\\) \\(\\gamma\\) k g + + 0 -1 + - \\(-2 \\pi\\) +1 - + \\(- \\pi\\) +1 - - \\(- \\pi\\)$ -1 These calculations were made with the cosinor model seen above. # Amplitude amp &lt;- sqrt(beta^2 + gamma^2) # Acrophase (phi) must be in correct quadrant sb &lt;- sign(beta) sg &lt;- sign(gamma) theta &lt;- atan(abs(gamma / beta)) if ((sb == 1 | sb == 0) &amp; sg == 1) { phi &lt;- -theta } else if (sb == -1 &amp; (sg == 1 | sg == 0)) { phi &lt;- theta - pi } else if ((sb == -1 | sb == 0) &amp; sg == -1) { phi &lt;- -theta - pi } else if (sb == 1 &amp; (sg == -1 | sg == 0)) { phi &lt;- theta - (2 * pi) } cat(paste0(&quot;Amplitude = &quot;, round(amp, 3))) ## Amplitude = 0.168 cat(paste0(&quot;Acrophase = &quot;, round(phi, 3))) ## Acrophase = -2.676 "],
["population-mean-cosinor.html", "4.3 Population-mean cosinor", " 4.3 Population-mean cosinor Based on the work by Cornelissen et al 2014 (Cornelissen 2014), the population mean cosinor can be estimated by applying the single or multiple component cosinor to each individual. \\[\\{\\hat{u} = \\hat{M}_{i} + \\hat\\beta_{i} + \\hat\\gamma_{i} + ... \\}\\] Where \\(i = 1, 2, ..., k\\) for each individual contribution to the population cosinor metrics. Each parameter can then be “averaged” to estimate the population parameters. This allows extension from a single individual to populations, particularly research studies with cohorts of patients. The \\(A\\) and \\(\\phi\\) however are calculated using the previous equations but through the \\(\\mu_{\\beta}\\) and \\(\\mu_{\\gamma}\\) values. The MESOR can be calculated simply by measure the mean value from each sample (\\(MESOR_{population} = MESOR_{1} + ... + MESOR_{k}\\)). # Parameters for population mean cosinor, using best datasets df &lt;- twins %&gt;% filter(med_beta_blockers != 1) %&gt;% select(c(&quot;rDYX&quot;, &quot;hour&quot;, &quot;patid&quot;)) names(df) &lt;- c(&quot;y&quot;, &quot;t&quot;, &quot;pop&quot;) highCounts &lt;- df %&gt;% group_by(pop) %&gt;% tally() %&gt;% filter(n &gt; 20) # Subset for full data df &lt;- subset(df, pop %in% highCounts$pop) # Number of individuals k &lt;- length(unique(df$pop)) # Individual cosinor models are implemented for each individual kCosinors &lt;- with( df, by(df, pop, function(x) { cosinor(y ~ t, data = x, tau = 24) }) ) # The coefficients have to be extracted and summarized tbl &lt;- sapply(kCosinors, stats::coef) coef_names &lt;- c(&quot;mesor&quot;, &quot;amp&quot;, &quot;phi&quot;, &quot;beta&quot;, &quot;gamma&quot;) rownames(tbl) &lt;- coef_names xmat &lt;- t(tbl) # Get mean for each parameter (mesor, beta, gamma), ignoring averaged amp/phi coefs &lt;- apply(xmat, MARGIN = 2, function(x) { sum(x) / k }) mesor &lt;- unname(coefs[&quot;mesor&quot;]) beta &lt;- unname(coefs[&quot;beta&quot;]) gamma &lt;- unname(coefs[&quot;gamma&quot;]) # Get amplitude amp &lt;- sqrt(beta^2 + gamma^2) # Acrophase (phi) must be in correct quadrant sb &lt;- sign(beta) sg &lt;- sign(gamma) theta &lt;- atan(abs(gamma / beta)) if ((sb == 1 | sb == 0) &amp; sg == 1) { phi &lt;- -theta } else if (sb == -1 &amp; (sg == 1 | sg == 0)) { phi &lt;- theta - pi } else if ((sb == -1 | sb == 0) &amp; sg == -1) { phi &lt;- -theta - pi } else if (sb == 1 &amp; (sg == -1 | sg == 0)) { phi &lt;- theta - (2 * pi) } # Update coefficients coefs[&quot;amp&quot;] &lt;- amp coefs[&quot;phi&quot;] &lt;- phi # Updated coefficients names(coefs) &lt;- coef_names print(coefs) ## mesor amp phi beta gamma ## 2.9020896 0.3144840 -2.6948505 -0.2836203 0.1358664 4.3.1 Confidence Intervals for Population Cosinor The confidence intervals for a population are more complicated to generate, and several approaches are documented in the literature. 4.3.1.1 Ellipsoid Approach The values, including standard deviation and standard error for the MESOR are calculated using standard statistics along a t-distribution, with degree of freedom based on number of observations. In this case, \\(\\alpha = 0.05\\). # Standard error for mesor kcoefs &lt;- data.frame(xmat) se &lt;- sd(kcoefs$mesor) / sqrt(k - 1) cat(round(se, 3)) ## 0.022 The statistical parameters around the \\(A\\) and \\(\\phi\\) are more complex, as they are joined together, and represent a joint confidence region of the substitute parameters \\(\\beta\\) and \\(\\gamma\\). The first step is the calculation of the variance and covariance of \\(\\beta\\) and \\(\\gamma\\). This can be used to generated teh standard deviation of these variables. \\[ \\begin{aligned} \\sigma_{\\beta \\gamma} &amp;= \\sqrt{COV_{\\beta \\gamma}} \\\\ \\sigma_{\\beta} &amp;= \\sqrt{VAR_{\\beta}} \\\\ \\sigma_{\\gamma} &amp;= \\sqrt{VAR_{\\gamma}} \\\\ \\end{aligned} \\] sbg &lt;- sqrt(cov(kcoefs$beta, kcoefs$gamma)) sb &lt;- sqrt(var(kcoefs$beta)) sg &lt;- sqrt(var(kcoefs$gamma)) The next step is the creation of a confidence ellipse for a given confidence interval. This ellipse is defined by all points \\((\\beta*, \\gamma*)\\) that satisfy the elliptical equation. \\[ \\frac{(\\beta - \\beta*)^2}{\\sigma^2_{\\beta}} - \\frac{2r(\\beta - \\beta*)(\\gamma - \\gamma*)}{\\sigma_{\\beta} \\sigma_{\\gamma}} + \\frac{(\\gamma - \\gamma*)}{\\sigma^2_{\\gamma}} = \\frac{2(1 - r^2)(k - 1)F_{1 - \\alpha}}{k(k - 2)} \\] This can be reorganized/reorderd by solving for a single parameter first, such as \\(\\beta*\\), which will lead to two potential values. \\[ \\beta* = \\frac{ \\beta \\sigma_{\\gamma} - r \\sigma_{beta} \\gamma + r \\sigma_{beta} \\gamma* \\pm \\sqrt{(r^2 - 1)[(\\gamma* - \\gamma)^2 - \\frac{2(k-1)}{k(k-2)}F_{1-\\alpha}\\sigma^2_{\\gamma}]} } {\\sigma_{\\gamma}} \\] … where \\(r = \\frac{\\sigma_{\\beta \\gamma}}{\\sigma_{\\beta} \\sigma_{\\gamma}}\\) This is calculated using the above equation for a potential sequence of values of \\(\\beta*\\) and \\(\\gamma*\\). THe constant values are already known to us, including the \\(\\beta\\) and \\(\\gamma\\) variables. # Variance/covariance and initial values were found above # Define new constants alpha &lt;- 0.05 r &lt;- sbg / (sb * sg) fstat &lt;- qf(1 - alpha, 2, k - 2) # Sequence values gseq &lt;- seq(from = -abs(gamma*3), to = abs(gamma*3), length.out = 100) bpos &lt;- ((beta * sg) - (r * sb * gamma) + (r * sb * gseq) + (sb * sqrt(as.complex((r^2 - 1) * ((gseq - gamma)^2 - ((2 * (k - 1)) / (k * (k - 2)) * fstat * sg^2)))))) / sg bneg &lt;- ((beta * sg) - (r * sb * gamma) + (r * sb * gseq) - (sb * sqrt(as.complex((r^2 - 1) * ((gseq - gamma)^2 - ((2 * (k - 1)) / (k * (k - 2)) * fstat * sg^2)))))) / sg # Restrict to only real numbers (not complex/imaginary) index &lt;- Im(bpos) == 0 | Im(bpos) == Im(bneg) # values are zero in both are REAL numbers gseq &lt;- Re(gseq[index]) bpos &lt;- Re(bpos[index]) bneg &lt;- Re(bneg[index]) # Plot out ellipse ggplot() + # Original values geom_point(aes(x = gamma, y = beta), data = kcoefs, alpha = 0.2) + # Potential ellipse versus hyperbola geom_point(aes(x = gseq, y = bpos), col = &quot;red&quot;, size = 0.5) + geom_point(aes(x = gseq, y = bneg), col = &quot;blue&quot;, size = 0.5) + # Predicted segment geom_segment(aes(x = 0, y = 0, xend = -amp*sin(phi), yend = amp*cos(phi)), size = 1.5) + # Axes geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + xlim(-abs(gamma)*5, abs(gamma)*5) + ylim(-abs(beta)*5, abs(beta)*5) ## Warning: Removed 7 rows containing missing values (geom_point). ## Warning: Removed 33 rows containing missing values (geom_point). # Using {car} border &lt;- car::dataEllipse(cbind(kcoefs$gamma, kcoefs$beta), levels = 0.95) %&gt;% as_tibble() ggplot() + geom_point(aes(x = x, y = y), data = border, col = &quot;red&quot;) + geom_point(aes(x = gamma, y = beta), data = kcoefs, alpha = 0.5) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) 4.3.1.2 Sampling Matrix Approach An approach, according to Bingham et al 1982, is to use the sampling matrix, generated from the following formulas and calculated below. The key formulas for the population cosinor confidence intervals are: $$ \\begin{aligned} MESOR &amp; \\ A &amp;t_{1 - /2} \\ &amp;+ arctan() \\end{aligned} $$ Where the matrix variables are shown below: $$ \\begin{aligned} s_{22} &amp;= \\ s_{23} &amp;= \\ s_{33} &amp;= \\end{aligned} $$ Thus, we can use these equations to calculate the confidence intervals. # Stats alpha &lt;- 0.05 tdist &lt;- qt(1 - alpha/2, k - 1) # Matrix variables s22 &lt;- ((sb^2 * beta^2) + (2 * sbg * beta * gamma) + (sg^2 * gamma^2)) / (k * amp^2) s23 &lt;- (-1 * (sb^2 - sg^2) * (beta * gamma) + sbg * (beta^2 - gamma^2)) / (k * amp^2) s33 &lt;- ((sb^2 * gamma^2) - (2 * sbg * beta * gamma) + (sg^2 * beta^2)) / (k * amp^2) 4.3.1.3 Approach by Fernandez (Fernández, Mojón, and Hermida 2004) The population aproach can also be predicted through an alternative, perhaps more intuitive way. If normality is assumed, the estimated parameters can be generated from the individual parameters, similar to the MESOR, in a single population, and allows for simple statistical testing between populations. (Fernández, Mojón, and Hermida 2004) # Stats kcoefs &lt;- data.frame(xmat) alpha &lt;- 0.05 tdist &lt;- qt(1 - alpha/2, k - 1) # Plot g &lt;- ggplot() + geom_segment( aes( x = gamma - (tdist * sd(kcoefs$gamma) / sqrt(k)), xend = gamma + (tdist * sd(kcoefs$gamma) / sqrt(k)), y = 0, yend = 0 ), col = &quot;cornflowerblue&quot;, size = 2 ) + geom_segment( aes( y = beta - (tdist * sd(kcoefs$beta) / sqrt(k)), yend = beta + (tdist * sd(kcoefs$beta) / sqrt(k)), x = 0, xend = 0 ), col = &quot;indianred&quot;, size = 2 ) + geom_rect( aes( ymin = beta - (tdist * sd(kcoefs$beta) / sqrt(k)), ymax = beta + (tdist * sd(kcoefs$beta) / sqrt(k)), xmin = 0, xmax = gamma + (tdist * sd(kcoefs$gamma) / sqrt(k)) ), fill = &quot;indianred&quot;, alpha = 0.5 ) + geom_rect( aes( ymin = 0, ymax = beta - (tdist * sd(kcoefs$beta) / sqrt(k)), xmin = gamma - (tdist * sd(kcoefs$gamma) / sqrt(k)), xmax = gamma + (tdist * sd(kcoefs$gamma) / sqrt(k)) ), fill = &quot;cornflowerblue&quot;, alpha = 0.5 ) + geom_point(aes(x = gamma, y = beta), size = 2) + geom_segment( aes( x = 0, y = 0, xend = -amp*sin(phi), yend = amp*cos(phi) ), size = 1.2 ) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) # Values possible for amplitude is.logical(round(kcoefs$amp * cos(kcoefs$phi), 3) == round(kcoefs$beta, 3)) ## [1] TRUE # Values for phi is.logical(round(kcoefs$amp * -1 * sin(kcoefs$phi), 3) == round(kcoefs$gamma, 3)) ## [1] TRUE sd(kcoefs$beta) ## [1] 0.3023976 sd(kcoefs$gamma) ## [1] 0.2376721 However, this method is more complicated when multiple components are included. References "],
["multiple-component-cosinor.html", "4.4 Multiple-Component Cosinor", " 4.4 Multiple-Component Cosinor Fitting physiological/circadian data may involve other patterns than a single, 24-hour frequency. There may be additional components or cosine waves that better explain the datasets, such as at 8 hours (e.g. meal times). Thus, it can be beneficial to add a secondary component. Implementing this in R is made complex as it now uses a variable number of inputs and variable number of outputs. This is performed through the paradigm suggested in the hardhat package, with a user-facing and computational side. The input data is given in the function call, then bridged to the model implementation. \\[ \\begin{aligned} y(t) &amp;= M + \\sum_{j}(A_{j} cos(2 \\pi t / \\tau_{j} + \\phi_{j}) \\\\ y(t) &amp;= M + \\beta_{j} x_{j} + \\gamma_{j} z_{j} \\end{aligned} \\] Where \\(j = 1,\\ 2,\\ ... p\\), which is the number of parameters. In a single component cosinor, there are at 3 parameters: \\(2(p=1) + 1\\), and in a multiple component: \\(2(p=j) + 1\\). The components are based on the periods. In a formula based approach… # This is the final dataset for a single component, however we have multiple object &lt;- cosinor(rDYX ~ hour, data = twins, tau = c(24, 8)) cat(object$call) ## cosinor(formula = rDYX ~ M + A1 * cos(2*pi*hour/24 + phi1) + A2 * cos(2*pi*hour/8 + phi2) # Periods tau &lt;- c(24, 8) # Two components l &lt;- length(tau) j &lt;- 2*l + 1 period &lt;- tau # No variable parameters y &lt;- outcomes &lt;- twins$rDYX t &lt;- predictors &lt;- twins$hour n &lt;- length(t) # Need to create number of x values to match number of periods # x1, x2, z1, z2 in this case for(i in 1:l) { assign(paste0(&quot;x&quot;, i), cos((2 * pi * t) / period[i])) assign(paste0(&quot;z&quot;, i), sin((2 * pi * t) / period[i])) } # Creating a new dataframe with all variables model &lt;- data.frame(y, t, mget(paste0(&quot;x&quot;, 1:l)), mget(paste0(&quot;z&quot;, 1:l))) # The formula, where the intercept will be the MESOR (not included) f &lt;- as.formula( paste0(&quot;y ~ &quot;, paste0(&quot;x&quot;, 1:l, &quot; + &quot;, &quot;z&quot;, 1:l, collapse = &quot; + &quot;)) ) print(f) ## y ~ x1 + z1 + x2 + z2 # Can create a model frame here using two approaches # Base R and with hardhat m &lt;- model.frame(f, model) xmat &lt;- model.matrix(f, m) ymat &lt;- as.matrix(y) # Hardhat framed &lt;- hardhat::model_frame(f, model) mat &lt;- hardhat::model_matrix(framed$terms, framed$data) # Solve for coefficients, including amplitude and acrophase coefs &lt;- solve(t(xmat) %*% xmat) %*% t(xmat) %*% ymat mesor &lt;- coefs[1] for(i in 1:l) { # Beta and gamma terms assign(paste0(&quot;beta&quot;, i), unname(coefs[paste0(&quot;x&quot;, i),])) assign(paste0(&quot;gamma&quot;, i), unname(coefs[paste0(&quot;z&quot;, i),])) # Amplitude assign(paste0(&quot;amp&quot;, i), sqrt(get(paste0(&quot;beta&quot;, i))^2 + get(paste0(&quot;gamma&quot;, i))^2)) # Phi / acrophase sb &lt;- sign(get(paste0(&quot;beta&quot;, i))) sg &lt;- sign(get(paste0(&quot;gamma&quot;, i))) theta &lt;- atan(abs(get(paste0(&quot;gamma&quot;, i)) / get(paste0(&quot;beta&quot;, i)))) if ((sb == 1 | sb == 0) &amp; sg == 1) { phi &lt;- -theta } else if (sb == -1 &amp; (sg == 1 | sg == 0)) { phi &lt;- theta - pi } else if ((sb == -1 | sb == 0) &amp; sg == -1) { phi &lt;- -theta - pi } else if (sb == 1 &amp; (sg == -1 | sg == 0)) { phi &lt;- theta - (2 * pi) } assign(paste0(&quot;phi&quot;, i), phi) } coefs &lt;- unlist(c(mesor = mesor, mget(paste0(&quot;amp&quot;, 1:l)), mget(paste0(&quot;phi&quot;, 1:l)), mget(paste0(&quot;beta&quot;, 1:l)), mget(paste0(&quot;gamma&quot;, 1:l)))) In a multiple-component cosinor analysis, if the periods are harmonic, as in if the longest value of \\(\\tau\\) is an integer multiple of the shortest \\(\\tau\\) (fundamental frequency), additional features can be extracted from the fit. \\(A_{g}\\) - global amplitude, defined as half of the difference between peak and trough values \\(\\phi_{O}\\) - orthophase, defined as lag time to peak value \\(\\phi_{B}\\) - bathyphase, defined as lag time to trough value This can be assessed through the fitted values in an augmented cosinor object. # Multiple component object object &lt;- cosinor(rDYX ~ hour, data = twins, tau = c(24, 12)) # Retrieve parameter values and fit aug &lt;- augment(object) fit &lt;- unique(aug[c(&quot;t&quot;, &quot;.fitted&quot;)]) mesor &lt;- object$coefficients[1] # Orthophase peak &lt;- max(fit$.fitted) orthophase &lt;- fit$t[which.max(fit$.fitted)] # Bathyphase trough &lt;- min(fit$.fitted) bathyphase &lt;- fit$t[which.min(fit$.fitted)] # Global amplitude globalAmp &lt;- (peak - trough) / 2 # Reference phase zero &lt;- min(aug$t) # Plot ggplot(fit, aes(x = t, y = .fitted)) + stat_smooth(method = &quot;gam&quot;, color = &quot;black&quot;, se = FALSE, size = 1.2) + # Mesor geom_hline(yintercept = mesor, color = &quot;grey&quot;) + geom_text(x = zero + 1, y = mesor + 0.01*mesor, label = &quot;MESOR&quot;) + # Orthophase geom_vline(xintercept = orthophase, color = &quot;grey&quot;) + geom_point(aes(x = orthophase, y = peak), size = 2) + geom_segment( aes(x = zero, xend = orthophase, y = peak, yend = peak), linetype = &quot;dotted&quot;, size = 0.8 ) + geom_text( aes(x = (orthophase - zero) / 2, y = peak + 0.01*mesor), label = &quot;Orthophase&quot; ) + # Bathyphase geom_vline(xintercept = bathyphase, color = &quot;grey&quot;) + geom_point(aes(x = bathyphase, y = trough), size = 2) + geom_segment( aes(x = zero, xend = bathyphase, y = trough, yend = trough), linetype = &quot;dotted&quot;, size = 0.5 ) + geom_text( aes(x = (bathyphase + zero) / 2, y = trough - 0.01*mesor), label = &quot;Bathyphase&quot; ) + # Global Amplitude geom_segment( aes(x = orthophase, xend = (orthophase + bathyphase)/2, y = peak, yend = peak), linetype = &quot;twodash&quot;, size = 0.5 ) + geom_segment( aes(x = bathyphase, xend = (orthophase + bathyphase)/2, y = trough, yend = trough), linetype = &quot;twodash&quot;, size = 0.5 ) + geom_segment( aes( x = (orthophase + bathyphase)/2, xend = (orthophase + bathyphase)/2, y = mesor, yend = trough ), linetype = &quot;twodash&quot;, size = 0.5, arrow = arrow(type = &quot;closed&quot;, length = unit(0.03, &quot;npc&quot;)) ) + geom_segment( aes( x = (orthophase + bathyphase)/2, xend = (orthophase + bathyphase)/2, y = mesor, yend = peak ), linetype = &quot;twodash&quot;, size = 0.5, arrow = arrow(type = &quot;closed&quot;, length = unit(0.03, &quot;npc&quot;)) ) + geom_text( aes(x = (bathyphase + orthophase)/2 + 4, y = 1.01*mesor), label = &quot;2 x Global Amplitude&quot; ) + theme_minimal() ## `geom_smooth()` using formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; "],
["heart.html", "Chapter 5 Heart", " Chapter 5 Heart As a physician, the mental model of disease is a human substrate that has been altered or damaged, such as a weakened heart after a myocardial infarction. Its an intuitive concept that is built on experience that stems from the underlying clinical research. That idea of mapping a disease onto a substrate allows for a certain simplicity in explaining a disease - physician’s teach via metaphor. The heart is metaphorically a house. It has rooms that are separated by doors, it has pipes in the walls, and electricity running through it. plumbing: epicardial and resistance vessels electric: sinoatrial and atrioventricular nodes and their corresponding fibers structure: the size and shape of the chambers, the valves, etc If an intuitive model such as this could be studied, would we make advances in cardiovascular medicine and research? Using the object-oriented programming approach with the S4 class in R, it will be explored. "],
["using-s4.html", "5.1 Using S4", " 5.1 Using S4 # First S4 method library(methods) heart &lt;- setClass( &quot;heart&quot;, slots = c( plumbing = &quot;list&quot;, electric = &quot;list&quot;, structural = &quot;list&quot; ) ) # Make a new object first &lt;- new( &quot;heart&quot;, plumbing = list(&quot;LHC&quot;, &quot;RHC&quot;), electric = list(&quot;SA&quot;, &quot;AV&quot;), structural = list(&quot;LA&quot;, &quot;RA&quot;, &quot;LV&quot;, &quot;RV&quot;) ) # Or, through the generator function second &lt;- heart( plumbing = list(&quot;LHC&quot;, &quot;RHC&quot;), electric = list(&quot;SA&quot;, &quot;AV&quot;), structural = list(&quot;LA&quot;, &quot;RA&quot;, &quot;LV&quot;, &quot;RV&quot;) ) "],
["package.html", "Chapter 6 Package Development", " Chapter 6 Package Development CRAN supports the publishing of open-source packages in R. The workflow on package development is improved by the following supporting packages: devtools pkgdown usethis testthat roxygen hardhat (creating modeling functions) In addition, git and Github are fundamental for version control in the development process. These resources were used in the development of my first package, card. "],
["documenting-a-package.html", "6.1 Documenting a Package", " 6.1 Documenting a Package The use of roxygen is fundamental in the process of package development, forcing explanatory variables and parameters to be documented as functions are developed. 6.1.1 Website The package pkgdown helps turn documentation into a visually attractive and navigable website. In addition, in the fashion of R, package logos are developed with a hexagon-framed sticker, representing a package. The development of a hex sticker is aided by the use of: https://github.com/GuangchuangYu/hexSticker http://connect.thinkr.fr/hexmake/ "],
["building-a-modeling-package.html", "6.2 Building a Modeling Package", " 6.2 Building a Modeling Package With the development of the card::cosinor() function, the tidy approach was inspired by the work of Max Kuhn’s hardhat package. THe basic concepts are: models require a user-facing interface models require an internal, computational interface these two interfaces must be bridged 6.2.1 Blueprints The idea of bridging the user-interface and the computational interface seems to be the birth of the hardhat::new_blueprint() concept, with its derivatives for standard user-facing methods (formulas, recipes, matrices, etc). "],
["functions.html", "Chapter 7 Functions", " Chapter 7 Functions The current approach philosophically in R is that everything is either a function or an object. A function, essentially a “predicate”, is used on any object, essentially a “noun”. This workflow of output &lt;- function(input) builds on the concepts of functional programming, and is a good intellectual/philosophical approach to most problems. In R, a good function: allows for some level of flexibility in input with stability of output allows for back-ends to improve performance/functionality without impacting the function user is extensible "],
["speed-of-for-loops.html", "7.1 Speed of for loops", " 7.1 Speed of for loops Here is an issue with a function using for loops. Generally, if there is an iterative process internal to the function, it will have an \\(~O(N^2)\\) performance. The card::recur_survival_table is built with intuitive for loops, at a significant cost with increased N sizes. Here is the performance of the function: test replications elapsed relative user.self sys.self user.child sys.child 4 15000 1 901.169 618.086 882.081 18.467 0 0 3 5000 1 115.573 79.268 112.617 2.942 0 0 1 100 1 1.458 1.000 1.448 0.009 0 0 2 1000 1 15.268 10.472 15.234 0.036 0 0 ## [1] &quot;\\n test replications elapsed relative user.self sys.self user.child sys.child\\n4\\t\\t 15000\\t\\t\\t\\t\\t\\t1 901.169 618.086 882.081 18.467 0 0\\n3 \\t\\t5000 1 115.573 79.268 112.617 2.942 0 0\\n1 \\t\\t 100\\t 1 1.458 1.000 1.448 0.009 0 0\\n2 \\t\\t1000\\t\\t 1 15.268 10.472 15.234 0.036 0 0\\n&quot; The function currently relies on for loops througout, as seen below: print(card::recur_survival_table) ## function (data, id, first, last, event.dates, model.type, death = NULL) ## { ## if (is.null(death)) { ## df &lt;- data[c(id, first, last, event.dates)] ## death &lt;- &quot;null_death&quot; ## df$null_death &lt;- 0 ## } ## else { ## df &lt;- data[c(id, first, last, event.dates, death)] ## } ## n &lt;- 0:length(event.dates) ## events &lt;- paste0(&quot;EVENT_DATE_&quot;, c(1:(length(n) - 1))) ## x &lt;- df[c(id, event.dates)] %&gt;% tidyr::pivot_longer(-c(dplyr::all_of(id)), ## names_to = &quot;EVENT&quot;, values_to = &quot;DATE&quot;) %&gt;% dplyr::group_by_(dplyr::all_of(id)) %&gt;% ## dplyr::arrange(DATE) %&gt;% dplyr::arrange_(dplyr::all_of(id)) %&gt;% ## tidyr::nest() ## for (i in 1:length(x[[id]])) { ## if (!plyr::empty(x[[i, 2]][[1]][duplicated(x[[i, 2]][[1]]$DATE), ## ])) { ## x[[i, 2]][[1]][duplicated(x[[i, 2]][[1]]$DATE), ]$DATE &lt;- NA ## } ## x[[i, 2]][[1]] %&lt;&gt;% dplyr::arrange(DATE) ## x[[i, 2]][[1]]$EVENT &lt;- events ## } ## df &lt;- tidyr::unnest(x, cols = data) %&gt;% tidyr::pivot_wider(names_from = &quot;EVENT&quot;, ## values_from = &quot;DATE&quot;) %&gt;% dplyr::inner_join(df[c(id, ## first, last, death)], ., by = id) ## names(df) &lt;- c(&quot;ID&quot;, &quot;FIRST&quot;, &quot;LAST&quot;, &quot;DEATH&quot;, events) ## df$EVENT_DATE_0 &lt;- df$FIRST ## for (i in 1:length(n)) { ## df$EVENT_DATE_0[!is.na(df[paste0(&quot;EVENT_DATE_&quot;, n[i])])] &lt;- df[[paste0(&quot;EVENT_DATE_&quot;, ## n[i])]][!is.na(df[paste0(&quot;EVENT_DATE_&quot;, n[i])])] ## } ## x &lt;- df[c(&quot;ID&quot;, paste0(&quot;EVENT_DATE_&quot;, n))] %&gt;% tidyr::pivot_longer(-c(&quot;ID&quot;), ## names_to = &quot;EVENT&quot;, values_to = &quot;DATE&quot;) %&gt;% stats::na.omit() %&gt;% ## dplyr::left_join(df, ., by = &quot;ID&quot;) ## switch(model.type, marginal = { ## x$STATUS &lt;- x$TSTART &lt;- x$TSTOP &lt;- 0 ## for (i in 2:length(n)) { ## x$TSTOP[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] &lt;- x[[paste0(&quot;EVENT_DATE_&quot;, ## n[i])]][x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] - ## x$FIRST[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] ## } ## x$TSTOP[x$EVENT == &quot;EVENT_DATE_0&quot;] &lt;- x$LAST[x$EVENT == ## &quot;EVENT_DATE_0&quot;] - x$FIRST[x$EVENT == &quot;EVENT_DATE_0&quot;] ## x$STATUS[x$EVENT == &quot;EVENT_DATE_0&quot; &amp; x$DEATH == 1] &lt;- 1 ## x$STATUS[x$EVENT != &quot;EVENT_DATE_0&quot;] &lt;- 1 ## }, pwptt = { ## x$STATUS &lt;- x$TSTART &lt;- x$TSTOP &lt;- 0 ## x$TSTOP[x$EVENT == &quot;EVENT_DATE_0&quot;] &lt;- x$LAST[x$EVENT == ## &quot;EVENT_DATE_0&quot;] - x$FIRST[x$EVENT == &quot;EVENT_DATE_0&quot;] ## for (i in 2:length(n)) { ## x$TSTOP[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] &lt;- x[[paste0(&quot;EVENT_DATE_&quot;, ## n[i])]][x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] - ## x$FIRST[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] ## } ## x$TSTART[x$EVENT == &quot;EVENT_DATE_0&quot;] &lt;- x$EVENT_DATE_0[x$EVENT == ## &quot;EVENT_DATE_0&quot;] - x$FIRST[x$EVENT == &quot;EVENT_DATE_0&quot;] ## for (i in 2:length(n)) { ## if (n[i] == 1) { ## x$TSTART[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] &lt;- 0 ## } else { ## x$TSTART[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] &lt;- as.numeric(unlist(x[x$EVENT == ## paste0(&quot;EVENT_DATE_&quot;, n[i]), paste0(&quot;EVENT_DATE_&quot;, ## n[i - 1])] - x[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, ## n[i]), &quot;FIRST&quot;])) ## } ## } ## x$STATUS[x$EVENT == &quot;EVENT_DATE_0&quot; &amp; x$DEATH == 1] &lt;- 1 ## x$STATUS[x$EVENT != &quot;EVENT_DATE_0&quot;] &lt;- 1 ## }, pwpgt = { ## x$STATUS &lt;- x$TSTART &lt;- x$TSTOP &lt;- 0 ## x$TSTOP[x$EVENT == &quot;EVENT_DATE_0&quot;] &lt;- x$LAST[x$EVENT == ## &quot;EVENT_DATE_0&quot;] - x$FIRST[x$EVENT == &quot;EVENT_DATE_0&quot;] ## for (i in 2:length(n)) { ## x$TSTOP[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] &lt;- x[[paste0(&quot;EVENT_DATE_&quot;, ## n[i])]][x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] - ## x$FIRST[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] ## } ## x$TSTART[x$EVENT == &quot;EVENT_DATE_0&quot;] &lt;- x$EVENT_DATE_0[x$EVENT == ## &quot;EVENT_DATE_0&quot;] - x$FIRST[x$EVENT == &quot;EVENT_DATE_0&quot;] ## for (i in 2:length(n)) { ## if (n[i] == 1) { ## x$TSTART[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] &lt;- 0 ## } else { ## x$TSTART[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, n[i])] &lt;- as.numeric(unlist(x[x$EVENT == ## paste0(&quot;EVENT_DATE_&quot;, n[i]), paste0(&quot;EVENT_DATE_&quot;, ## n[i - 1])] - x[x$EVENT == paste0(&quot;EVENT_DATE_&quot;, ## n[i]), &quot;FIRST&quot;])) ## } ## } ## x$STATUS[x$EVENT == &quot;EVENT_DATE_0&quot; &amp; x$DEATH == 1] &lt;- 1 ## x$STATUS[x$EVENT != &quot;EVENT_DATE_0&quot;] &lt;- 1 ## x$TSTOP &lt;- x$TSTOP - x$TSTART ## x$TSTART &lt;- x$TSTART - x$TSTART ## }, stop(&quot;Need the correct repeat event model: marginal, pwptt, pwpgt&quot;)) ## y &lt;- x[c(&quot;ID&quot;, &quot;TSTART&quot;, &quot;TSTOP&quot;, &quot;STATUS&quot;, &quot;EVENT&quot;, &quot;DATE&quot;)] ## return(y) ## } ## &lt;bytecode: 0x7fe30ab5c5c0&gt; ## &lt;environment: namespace:card&gt; "],
["models.html", "Chapter 8 Modeling", " Chapter 8 Modeling Here is a collection of explorations on modeling approaches as they related to electrocardiography and epidemiology. "],
["modeling-multiple-outcomes-and-predictors.html", "8.1 Modeling Multiple Outcomes and Predictors", " 8.1 Modeling Multiple Outcomes and Predictors A recurrent issue with causality-focused modeling with ECG data is that there are multiple outcomes (different ECG features). For example, in the card package, the geh dataset contains several ECG features based on vectorcardiography. 8.1.1 Creating Multiple Models library(card) library(tidyverse) data(geh) names(geh) ## [1] &quot;pid&quot; &quot;hhp_id&quot; &quot;age&quot; ## [4] &quot;sex&quot; &quot;age_cat&quot; &quot;systolic_bp_first&quot; ## [7] &quot;systolic_bp_second&quot; &quot;systolic_bp_third&quot; &quot;diastolic_bp_first&quot; ## [10] &quot;diastolic_bp_second&quot; &quot;diastolic_bp_third&quot; &quot;pulse_rate_first&quot; ## [13] &quot;pulse_rate_second&quot; &quot;height_cm&quot; &quot;weight_kg&quot; ## [16] &quot;waist_cm&quot; &quot;dia_trt_allopdrug&quot; &quot;hbp_trt_allopdrug&quot; ## [19] &quot;hyp_trt_allopdrug&quot; &quot;lab_hba1c&quot; &quot;lab_fasting_bg&quot; ## [22] &quot;lab_fasting_insulin&quot; &quot;lab_tchol&quot; &quot;lab_ldlchol&quot; ## [25] &quot;lab_hdlchol&quot; &quot;lab_triglyc&quot; &quot;lab_ser_urea&quot; ## [28] &quot;lab_ser_creatinine&quot; &quot;lab_urin_malbumin&quot; &quot;pd_heart&quot; ## [31] &quot;bmi&quot; &quot;bmi_cat&quot; &quot;obese&quot; ## [34] &quot;obese_asian&quot; &quot;sbp_mean&quot; &quot;dbp_mean&quot; ## [37] &quot;pulse_mean&quot; &quot;htn&quot; &quot;cad&quot; ## [40] &quot;drugs_dm&quot; &quot;dm&quot; &quot;dm_lab&quot; ## [43] &quot;dm_control&quot; &quot;dm_pre&quot; &quot;homa&quot; ## [46] &quot;high_waist&quot; &quot;high_tchol&quot; &quot;high_ldl&quot; ## [49] &quot;low_hdl&quot; &quot;high_triglyc&quot; &quot;met_syn_num&quot; ## [52] &quot;met_syn&quot; &quot;pr_interval&quot; &quot;p_duration&quot; ## [55] &quot;p_amp&quot; &quot;qrs_duration&quot; &quot;qt_interval&quot; ## [58] &quot;cornell_voltage&quot; &quot;nhanes_score&quot; &quot;svg_mag&quot; ## [61] &quot;az_svg&quot; &quot;az_svg_m&quot; &quot;el_svg&quot; ## [64] &quot;el_svg_m&quot; &quot;qrs_tang&quot; &quot;auc_vm_qt&quot; ## [67] &quot;wvg&quot; &quot;log_svg&quot; &quot;log_auc_qt&quot; ## [70] &quot;log_wvg&quot; The first issue is the causal model, which can be visualized using a directed acyclic graph. The variables of interest are a subset of the dataset. In this case, we’re looking at the relationship of diabetes with cardiotoxicity in a very small subset of participants. library(ggdag) ## ## Attaching package: &#39;ggdag&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## filter bd &lt;- dagify( GEH ~ DM + Age + BMI + HTN + CAD + IR + Sex, DM ~ Age + IR + BMI, CAD ~ DM + BMI + HTN + Age + Sex, HTN ~ Age, IR ~ BMI + Age, Age ~ Sex, exposure = &quot;DM&quot;, outcome = &quot;GEH&quot; ) d1 &lt;- ggdag_parents(bd, &quot;DM&quot;, layout = &quot;star&quot;) + theme_dag() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Factors Affecting Diabetes&quot;) d2 &lt;- ggdag_parents(bd, &quot;GEH&quot;, layout = &quot;star&quot;) + theme_dag() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Factors Affecting GEH&quot;) # Combine and plot gridExtra::grid.arrange(d1, d2, nrow = 1) As we can see, many things effect ECG findings, and a subgroup of those impact diabetes, suggesting a number of potential effect modifiers and potential confounders/mediators. Using a sequential model building method in the card package allows for a simple way to perform this analysis. This will build a linear model for each outcome, and repeat the model with an additional covariate in the sequence of listed in the formula. # Select variables vars &lt;- c(&quot;svg_mag&quot;, &quot;az_svg&quot;, &quot;el_svg&quot;, &quot;qrs_tang&quot;, &quot;log_auc_qt&quot;, &quot;log_wvg&quot;, &quot;lab_hba1c&quot;, &quot;lab_fasting_bg&quot;, &quot;homa&quot;, &quot;dm&quot;, &quot;age&quot;, &quot;bmi&quot;, &quot;bmi_cat&quot;, &quot;age_cat&quot;, &quot;sex&quot;, &quot;htn&quot;, &quot;cad&quot;, &quot;lab_ser_creatinine&quot;, &quot;lab_tchol&quot;) df &lt;- geh %&gt;% select(all_of(vars)) %&gt;% #na.omit() %&gt;% #filter(homa &lt;= 5 * sd(homa, na.rm = TRUE)) %&gt;% # Remove outliers mutate( bmi_cat = factor(bmi_cat, levels = c(0:3), labels = c(&quot;Underweight&quot;, &quot;Normal&quot;, &quot;Overweight&quot;, &quot;Obese&quot;)), age_cat = factor(age_cat, levels = c(0:2), labels = c(&quot;&lt;45&quot;, &quot;45-65&quot;, &quot;&gt;65&quot;)), sex = factor(sex, levels = c(0,1), labels = c(&quot;Female&quot;, &quot;Male&quot;)) ) %&gt;% mutate(across( c(svg_mag, az_svg, el_svg, qrs_tang, log_auc_qt, log_wvg), function(x) { as.vector(scale(x, center = TRUE, scale = TRUE)) } )) # Sequential model building models &lt;- card::build_sequential_models( svg_mag + az_svg + el_svg + qrs_tang + log_auc_qt + log_wvg ~ lab_hba1c + age + sex + bmi + cad + htn, data = df, exposure = &quot;lab_hba1c&quot;, engine = &quot;lm&quot; ) head(models) ## # A tibble: 6 x 9 ## outcomes term estimate std.error statistic p.value conf.low conf.high covar ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 svg_mag (Inter… 0.339 0.196 1.73 0.0847 -0.0466 0.724 1 ## 2 svg_mag lab_hb… -0.0412 0.0234 -1.76 0.0796 -0.0872 0.00488 1 ## 3 az_svg (Inter… -0.315 0.195 -1.62 0.107 -0.698 0.0681 1 ## 4 az_svg lab_hb… 0.0381 0.0233 1.64 0.102 -0.00763 0.0839 1 ## 5 el_svg (Inter… 0.0767 0.195 0.394 0.694 -0.306 0.460 1 ## 6 el_svg lab_hb… -0.00862 0.0233 -0.370 0.711 -0.0544 0.0371 1 8.1.2 Visualize Regression Estimates To assess or get a sense of how the variables are playing out, we can visualize the estimates across the builds of the models. This will also use the gganimate package to show the effect of data layering # Libraries library(gganimate) library(ggthemes) # Data df &lt;- models %&gt;% # Remove intercepts filter(term != &quot;(Intercept)&quot;) %&gt;% # Sequence the terms mutate( term = factor( term, levels = c(&quot;lab_hba1c&quot;, &quot;age&quot;, &quot;sexMale&quot;, &quot;bmi&quot;, &quot;cad1&quot;, &quot;htn1&quot;), labels = c(&quot;HbA1c&quot;, &quot;Age&quot;, &quot;Sex&quot;, &quot;BMI&quot;, &quot;CAD&quot;, &quot;HTN&quot;) ) ) # ggplot g &lt;- ggplot(df, aes(x = factor(covar), y = estimate, color = term)) + facet_wrap(~outcomes, scales = &quot;fixed&quot;) + geom_point( aes(color = term), data = filter(df, p.value &gt;= 0.20), shape = 1, position = &quot;jitter&quot; ) + geom_point( aes(color = term), data = filter(df, p.value &lt; 0.20), shape = 19, position = &quot;jitter&quot; ) + scale_color_ptol(name = &quot;Predictors&quot;) + theme_minimal() + theme( legend.position = &quot;bottom&quot;, legend.box = &quot;horizontal&quot;, panel.border = element_rect(colour = &quot;black&quot;, fill = NA) ) + labs( title = &quot;Estimates in Sequential Models&quot;, x = &quot;Number of Covariates in Model&quot;, y = &quot;GEH Parameters (z-normalized)&quot; ) # Animated a &lt;- g + transition_reveal(covar) animate(a, end_pause = 30) "],
["vcg.html", "Chapter 9 Vectorcardiography ", " Chapter 9 Vectorcardiography "],
["vector-gradients.html", "9.1 Vector Gradients", " 9.1 Vector Gradients Mark Josephson in 1988 found that repolarization became non-uniform post-infarction, which was the suggestive substrate of VT/VF. The dispersion of the total recovery time is suggestive of global electrical heterogeneity, which can then predict SCD. Vectorcardiography (VCG) characterizes the electrical heart vector movement through a cardiac cycle. This is understood best through the spatial ventricular gradient vector (SVG), as described by Frank Wilson in 1934, and expanded upon by J. Willis Hurst. This is different and independent of the sequence of ventricular activation, which can be seen on ECG. The work by Larisa Tereschchenko has helped to compute these concepts for analytical approaches (Tereshchenko 2018). The SVG points to different locations in healthy versus diseased hearts. Summary of SVG: Points along the direction of greatest activation and recovery time (which is perpendicular to the line of conduction block, such as scar) Points towards to the area where the total recovery time is the shortest Depends on the heterogeneity of action potential across entire myocardium Characterizes the degree of heterogeneity of recovery time across the ventricles Steepness of the gradient determines magnitude of the SVG (areas of contrasting recovery time thus give largest SVG) References "],
["global-electrical-heterogeneity.html", "9.2 Global Electrical Heterogeneity", " 9.2 Global Electrical Heterogeneity The SVG can be broken down in 5 VCG parameters to describe the overall global electrical heterogeneity of the heart, as seen in the Figure below (Waks et al. 2016). SVG magnitude SVG azimuth SVG elevation Spatial QRS-T angle = the three-dimensional angle between mean spatial QRS-vector and mean spatial T-vector, measured in degrees Sum absolute QRST integral = scalar analog of the SVG, calculated as absolute value under QRS cmplex and T-wave, measured in millivolts (integral of voltage over time) vector gradient References "],
["cardiac-catheterization.html", "Chapter 10 Cardiac Catheterization", " Chapter 10 Cardiac Catheterization A large part of cardiovascular studies of interest are hemodynamic parameters. These parameters are specific to the structure of the heart (e.g. valves closing and opening, the heart contracting, etc), and cycle over a single wave. These patterns are seen as different waveforms in different chambers of the heart: RA = 1-5 mm Hg RV = 25/5 mm Hg PA = 20/10 mm Hg LA = 3-12 mm Hg LV = 120/12 mm Hg The waveforms could theoretically be recreated using an understanding of intracardiac hemodynamics (the “structure follows function” appraoch). As these are cyclic, sinusoidal curves would be appropriate for describing these curves. "],
["right-heart-catheterization.html", "10.1 Right heart catheterization", " 10.1 Right heart catheterization There are several chambers that are measured in a right-heart study. Usually, the order is: Right atrium pressure Right ventricle pressure Pulmonary artery pressure Pulmonary capillary wedge pressure The right atrial waveform is mapped onto several events: Waveform Cardiac Cycle Mechanical Process a end diastole atrial contraction c early systole tricuspid bulge v late systole systolic atrial filling x mid systole atrial relaxation y early diastole early ventricular filling Each of these can be mapped onto a time or phase-shift of a single cycle. For now, \\(2 pi\\) will be a cycle length. # Setup library(ggplot2) # Time positions of each peak and descent tau &lt;- 2*pi xa &lt;- 2/16 * tau xc &lt;- 8/16 * tau xx &lt;- 10/16 * tau xv &lt;- 12/16 * tau xy &lt;- 14/16 * tau # Height of peaks and troughs ya &lt;- 8 yc &lt;- 6 yx &lt;- 2 yv &lt;- 10 yy &lt;- 1 # Data library(mgcv) ## Loading required package: nlme ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse ## This is mgcv 1.8-31. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. df &lt;- data.frame(x = c(xa, xc, xx, xv, xy), y = c(ya, yc, yx, yv, yy)) m &lt;- gamm(y ~ s(x, bs = &quot;cc&quot;, k = 3), data = df) ## Warning in smooth.construct.cc.smooth.spec(object, dk$data, dk$knots): basis dimension, k, increased to minimum possible plot(m$gam) This simulates a portion of a single descent, but is problematic interms of cyclic nature and repeated measures. An alternative approach may be to consider segments of the data that are delayed based on findings from ECG (e.g. p-wave for atrial contraction). "]
]
